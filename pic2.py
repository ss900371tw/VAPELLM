import streamlit as st
import requests
import os
import shutil
import time
import random
import re
from bs4 import BeautifulSoup
from dotenv import load_dotenv
from PIL import Image

from langchain_core.output_parsers import StrOutputParser
from langchain_core.messages import HumanMessage
from langchain_openai import ChatOpenAI
from langchain.prompts import PromptTemplate

from googleapiclient.discovery import build

# -------------------- 1. ç’°å¢ƒè®Šæ•¸ --------------------
load_dotenv()
openai_api_key = os.getenv("OPENAI_API_KEY","")


# -------------------- 2. Prompt --------------------
text_template = """
As an expert classifier of e-cigarette and controlled-substance-selling websites,
you are equipped to analyze the following HTML and categorize it as either:
(1) an e-cigarette/controlled-substance-selling website,
(2) not such a website.

Because there are infinitely many potential products, brand names, or references, you will use
general indicators rather than item-specific keywords. In particular, consider the following:

1) **Context of Use or Claims**:
   - Look for explicit or subtle references to psychoactive, mind-altering, or illicit effects.
   - Check if the product or substance is described as mood-altering, giving a "high," 
     relieving stress in an excessive/unusual manner, or otherwise hinting at controlled-substance properties.

2) **Transactional Cues**:
   - Mentions of pricing, "contact me directly," "secret formula," "LINE for details," or 
     "discrete shipping" may suggest illicit or regulated products.
   - Any site that allows **adding a controlled substance to cart**, submitting a **quick order**, or browsing a **product catalog** must be treated with caution, regardless of the overall appearance of legitimacy.

3) **Legal Categorization or Warnings**:
   - Check for restricted sale notes, age gating, disclaimers limiting sale to licensed buyers, or regional restrictions.
   - Be cautious of disclaimers that are vague or contradictory.

4) **Product or Service Type**:
   - **If a product is a known controlled substance or prescription medication** (e.g., Zopiclone, Diazepam, Fentanyl, Ketamine, Alprazolam, Tramadol, etc.), and is shown alongside features like:
     - "Add to Cart"
     - "Quick Order"
     - SKU or catalog number
     - Price lookup
     - Purchase instructions
     
     â†’ **You must classify it as (1)** â€” even if the site appears scientific, medical, or intended for research.

   - **âš ï¸ Scientific context or professional branding does NOT override the fact that the product is a controlled substance offered for sale.**

   - E-cigarette devices and e-liquids should also be classified as (1).

   - If the product is a non-controlled supplement, traditional medicine, or harmless herbal product, classify as (2).

5) **Critical Thought**:
   - Do NOT rely solely on appearance, branding, or stated purpose.
   - Prioritize the **actual product type** and **whether the website facilitates ordering** of a controlled substance.

6) Informational or Educational Context:
   - If the website only provides reference information about a drug (such as usage, side effects, interactions, dosages), and does not offer any purchase, order, or contact mechanism, it should be classified as (2).
   - These may include medical encyclopedias, health information platforms, government resources, or pharmacy guides (e.g., drugs.com, WebMD, NIH, or Mayo Clinic).
   - Even if controlled substances are discussed, the absence of ordering capability, pricing, product SKUs, or transactional elements indicates a non-commercial, informative nature.

ğŸ§  Be especially cautious not to misclassify informational reference sites as sellers.

---
HTML content to classify:
{html}

---
Comprehensive Report:
- Final classification: (1) or (2)
- What general indicators led to your conclusion (based on sections 1â€“6)?
- If there are any ambiguities (e.g., controlled substance on a professional-looking website), explain how you resolved them.
- **Emphasize the product type and orderability over context or appearance.**
"""
prompt = PromptTemplate.from_template(template=text_template)

# -------------------- 3. çˆ¬å–ç¶²é æ–‡å­— --------------------
def crawl_all_text(url: str):
    try:
        response = requests.get(url, timeout=10)
        response.raise_for_status()
        soup = BeautifulSoup(response.text, 'html.parser')
        return soup.get_text(separator="\n", strip=True)[:50]
    except requests.exceptions.RequestException as e:
        return f"[Request failed]: {e}"

# -------------------- 4. çˆ¬å–åœ–ç‰‡ --------------------
def crawl_images(url: str):
    try:
        response = requests.get(url, timeout=10)
        response.raise_for_status()
        soup = BeautifulSoup(response.text, 'html.parser')
        img_tags = soup.find_all("img")
        valid_extensions = {".jpg", ".jpeg", ".png", ".webp"}
        img_urls = []
        for img in img_tags:
            src = img.get("src")
            if not src:
                continue
            lower_src = src.lower()
            if any(lower_src.endswith(ext) for ext in valid_extensions):
                full_url = requests.compat.urljoin(url, src)
                img_urls.append(full_url)
        return img_urls
    except:
        return []

# -------------------- 5. ä¸‹è¼‰åœ–ç‰‡ --------------------
def download_image(img_url, save_path="images"):
    if not os.path.exists(save_path):
        os.makedirs(save_path)
    img_name = os.path.join(save_path, os.path.basename(img_url.split("?")[0]))
    try:
        response = requests.get(img_url, stream=True, timeout=10)
        response.raise_for_status()
        with open(img_name, "wb") as out_file:
            shutil.copyfileobj(response.raw, out_file)
        return img_name
    except:
        return None

# -------------------- 6. åˆ†æåœ–ç‰‡ --------------------
def get_image_prompt(img_url: str) -> str:
    return f"""
Analyze this image and determine whether it contains any of the following:
ğŸš¨ **If the image contains ANY of the following, classify it as "Warning: Contains restricted items"** ğŸš¨

- **E-cigarettes, vape pens, e-liquids, or any part of a vaping device** (including mouthpiece, tank, battery compartment, or any identifiable section of an e-cigarette).
- **Brand names or logos of vaping-related products** (even if not shown in promotional content).
- **Vape-related smoke effects** (clouds of vapor that indicate the presence of vaping activity).
- **Controlled substances** (such as marijuana, cocaine, heroin, pills, stimulants, or hallucinogens).
- **Paraphernalia associated with substance use** (rolling papers, bongs, syringes, grinders, etc.).
- **Psychoactive substances** (such as psychedelics, mushrooms, LSD, or DMT).
- **Visual elements commonly associated with drug culture** (e.g., "420" references, ğŸ symbols, psychedelic visuals).
- **Marketing materials promoting vaping products** (ads, discounts, promo banners, or promotional product displays).

âœ… **If NONE of the above are present, classify it as "Safe".**

Image URL: {img_url}
"""

def classify_image(img_url: str, model: ChatOpenAI):
    prompt_str = get_image_prompt(img_url)
    result = model.invoke([HumanMessage(content=prompt_str)])
    return result.content

# -------------------- 7. Google Search --------------------
def google_search(query, count=10):
    api_key = os.getenv("GOOGLE_API_KEY")
    cx = os.getenv("GOOGLE_CX")
    if not api_key or not cx:
        print("âŒ GOOGLE_API_KEY æˆ– GOOGLE_CX æ²’æœ‰æ­£ç¢ºè¨­å®š")
        return []
    try:
        service = build("customsearch", "v1", developerKey=api_key)
        results = []
        fetched = 0
        while fetched < count:
            num = min(10, count - fetched)
            start = fetched + 1
            res = service.cse().list(q=query, cx=cx, num=num, start=start).execute()
            items = res.get("items", [])
            results.extend([item["link"] for item in items])
            fetched += len(items)
            if len(items) < num:
                break
        return results
    except Exception as e:
        print(f"âŒ Google æœå°‹éŒ¯èª¤ï¼š{e}")
        return []

# -------------------- 8. é»‘åå–® --------------------
blacklist_domains = [
    ".edu", ".gov", ".ac.", ".org", ".wiki", "usask.ca", "su.se", "article",
    "researchgate", "sciencedirect", "osf.io", "digitalcommons",
    "escholarship", "openai.com", "archive.org", "wiktionary",
    "urbandictionary", "dictionary", "bjc-r", "ecprice", "adamrose"
]
blacklist_keywords_in_url = [
    "slang", "street-names", "code-words", "download", "vocab", "wordlist",
    "unigrams", "passphrases", "pdf", "xml", "djvu.txt", "txt", "books",
    "raw/main", "viewcontent.cgi", "novel.pdf", "API/docs", "textfiles",
    "publications"
]

def is_blacklisted_url(url: str) -> bool:
    url_lower = url.lower()
    return any(domain in url_lower for domain in blacklist_domains) or \
           any(kw in url_lower for kw in blacklist_keywords_in_url)

# -------------------- 9. Streamlit ä¸»ç¨‹å¼ --------------------
def main():
    st.title("é›»å­è¸ç¶²ç«™åµæ¸¬ç³»çµ±")

    st.markdown("""
    <style>
        .stApp {
            background-color: #fce4ec; /* æŸ”äº®çš„é¦™è•‰é»ƒ */
            padding-top: 2rem;
        }
        h1, h2, h3 {
            color: #00FFFF; /* Electric Blue */
        }
        .stRadio > div {
            flex-direction: row;
            gap: 2rem;
        }
        .stButton > button, .stDownloadButton > button {
            background-color: #3EB489; /* Mint Green */
            color: white;
            font-weight: bold;
            border-radius: 8px;
            padding: 0.5rem 1.2rem;
        }
        .stTextInput > div > input,
        .stTextArea > div > textarea {
            border-radius: 6px;
        }
    </style>
""", unsafe_allow_html=True)
    st.markdown("## ğŸ§  é›»å­è¸ç¶²ç«™åµæ¸¬ç³»çµ±")
    st.markdown("# åˆ©ç”¨ OpenAI + åœ–ç‰‡è¾¨è­˜ï¼Œè‡ªå‹•åˆ†é¡é›»å­ç…™ç›¸é—œç¶²ç«™")
    
    llm_text = ChatOpenAI(api_key=openai_api_key, model="gpt-4o", temperature=0)
    llm_image = ChatOpenAI(api_key=openai_api_key, model="gpt-4o", temperature=0)
    parser = StrOutputParser()
    chain = prompt | llm_text | parser

    mode = st.radio("# é¸æ“‡æ¨¡å¼ï¼š", ["å–®ä¸€ç¶²å€åˆ†æ", "æ‰¹é‡ç¶²å€åˆ†æ", "GOOGLE è‡ªå‹•æœå°‹ & åˆ†æ"])

    if mode == "å–®ä¸€ç¶²å€åˆ†æ":
        st.markdown("### ğŸ”— å–®ä¸€ç¶²å€åˆ†æ")
        url = st.text_input("è«‹è¼¸å…¥ç¶²å€ï¼š")

        if st.button("ğŸš€ é–‹å§‹åˆ†æ"):
            if not url.strip():
                st.warning("âš ï¸ è«‹è¼¸å…¥æœ‰æ•ˆç¶²å€")
                return

            st.markdown(f"### ğŸ” æ­£åœ¨åˆ†æï¼š[{url}]({url})")

            with st.spinner("â³ æ­£åœ¨è®€å–ç¶²ç«™å…§å®¹èˆ‡åœ–ç‰‡..."):
                text_content = crawl_all_text(url)
                text_result = chain.invoke(text_content)

                image_urls = crawl_images(url)
                flagged_images = 0

                # åˆ†æˆå…©æ¬„é¡¯ç¤ºåˆ†æçµæœ
                col1,  col2 = st.columns([5,  5])

                with col1:
                    st.markdown("#### ğŸ“„ æ–‡å­—åˆ†é¡çµæœ")
                    st.write(text_result)

                with col2:
                    st.markdown("#### ğŸ“· åœ–åƒåˆ†æçµæœ")
                    if not image_urls:
                        st.write("(æœªæ‰¾åˆ°åœ–ç‰‡)")
                    else:
                        sample_size = min(2, len(image_urls))
                        for img in random.sample(image_urls, sample_size):
                            img_result = classify_image(img, llm_image)
                            st.image(img, caption=f"åˆ†é¡çµæœ: {img_result}")
                            if "Warning" in img_result:
                                flagged_images += 1

            # ç¶œåˆçµè«–
            st.markdown("---")
            st.subheader("ğŸ“‹ ç¶œåˆçµè«–")
            if "(1)" in text_result or flagged_images > 0:
                st.error("âš ï¸ é«˜é¢¨éšªç¶²ç«™ï¼šç¶²ç«™å¯èƒ½æ¶‰åŠé›»å­ç…™æˆ–ç®¡åˆ¶è—¥å“è²©å”®")
            else:
                st.success("âœ… å®‰å…¨ç¶²ç«™ï¼šæœªåµæ¸¬å‡ºé«˜é¢¨éšªå…§å®¹")

    elif mode == "æ‰¹é‡ç¶²å€åˆ†æ":
        st.markdown("### ğŸ“‚ æ‰¹é‡ç¶²å€åˆ†æ")
        uploaded_file = st.file_uploader("è«‹ä¸Šå‚³ `.txt` æª”æ¡ˆï¼ˆæ¯è¡Œä¸€å€‹ç¶²å€ï¼‰", type=["txt"])

        if st.button("ğŸš€ é–‹å§‹æ‰¹æ¬¡åˆ†æ"):
            if uploaded_file is None:
                st.warning("âš ï¸ è«‹å…ˆä¸Šå‚³ `.txt` æª”æ¡ˆ")
                return

            urls = [line.strip().decode("utf-8") for line in uploaded_file.readlines() if line]
            st.info(f"ğŸ“„ å…±æœ‰ {len(urls)} å€‹ç¶²å€å°‡é€²è¡Œåˆ†æ")

            high_risk_urls = []

            for idx, url in enumerate(urls, start=1):
                st.markdown(f"---\n### ğŸ”— [{idx}/{len(urls)}] åˆ†æç¶²å€ï¼š[{url}]({url})")

                with st.spinner("â³ æ­£åœ¨åˆ†æ..."):
                    text_content = crawl_all_text(url)
                    text_result = chain.invoke(text_content)
                    image_urls = crawl_images(url)
                    flagged_images = 0

                    # å·¦å³åˆ†å€ï¼šæ–‡å­— / åœ–åƒ
                    col1,  col2 = st.columns([5, 5])


                    with col1:
                        st.markdown("#### ğŸ“„ æ–‡å­—åˆ†é¡çµæœ")
                        st.write(text_result)


                    with col2:
                        st.markdown("#### ğŸ“· åœ–åƒåˆ†æçµæœ")
                        if not image_urls:
                            st.write("(æœªæ‰¾åˆ°åœ–ç‰‡)")
                        else:
                            for img in image_urls[:2]:
                                img_result = classify_image(img, llm_image)
                                st.image(img, caption=f"åˆ†é¡çµæœ: {img_result}")
                                if "Warning" in img_result:
                                    flagged_images += 1

                # ç¶œåˆçµè«–
                if "(1)" in text_result or flagged_images > 0:
                    high_risk_urls.append(url)
                    st.error("âš ï¸ é«˜é¢¨éšªç¶²ç«™")
                else:
                    st.success("âœ… å®‰å…¨ç¶²ç«™")

            st.markdown("---")
            st.subheader("ğŸ“‹ æ‰¹æ¬¡åˆ†æç¸½çµ")

            if high_risk_urls:
                st.warning(f"âš ï¸ å…±åµæ¸¬åˆ°é«˜é¢¨éšªç¶²å€ {len(high_risk_urls)} ç­†")

                st.download_button(
                    label="ğŸ“¥ ä¸‹è¼‰é«˜é¢¨éšªç¶²å€æ¸…å–®",
                    data="\n".join(high_risk_urls),
                    file_name="high_risk_urls.txt",
                    mime="text/plain"
                )
            else:
                st.success("âœ… æ‰€æœ‰ç¶²å€çš†æœªåµæ¸¬åˆ°é«˜é¢¨éšªå…§å®¹")

    else:
        st.markdown("### ğŸŒ Google æœå°‹åˆ†ææ¨¡å¼")
        st.markdown("> æ ¹æ“šé—œéµå­—è‡ªå‹•æœå°‹ç¶²ç«™ï¼Œä¸¦å°æ¯å€‹æœå°‹çµæœé€²è¡Œ AI æ–‡æœ¬èˆ‡åœ–åƒåˆ¤æ–·")

        # è¼¸å…¥é—œéµå­—
        keywords_text = st.text_area(
            "ğŸ”¤ è«‹è¼¸å…¥æœå°‹é—œéµå­—ï¼ˆæ¯è¡Œä¸€å€‹ï¼‰",
            "vape\ne-juice\ne-cigarette\né›»å­ç…™"
        )

        limit = st.number_input("ğŸ”¢ æ¯å€‹é—œéµå­—æœ€å¤šæ“·å–å¹¾çµ„ç¶²å€ï¼Ÿ", min_value=1, max_value=50, value=10)

        if st.button("ğŸš€ åŸ·è¡Œ Google æœå°‹ä¸¦åˆ†æ"):
            if not keywords_text.strip():
                st.warning("âš ï¸ è«‹å…ˆè¼¸å…¥é—œéµå­—")
                return

            keywords_list = [kw.strip() for kw in keywords_text.split("\n") if kw.strip()]
            st.info(f"ğŸ” å°‡é‡å° {len(keywords_list)} å€‹é—œéµå­—ï¼Œå„æ“·å– {limit} çµ„æœå°‹çµæœ")

            all_urls = []
            for kw in keywords_list:
                st.markdown(f"#### ğŸ” æœå°‹é—œéµå­—ï¼š**{kw}**")
                found = google_search(kw, count=limit)
                all_urls.extend([url for url in found if url not in all_urls])

            st.write(f"ğŸ“¥ ç¸½å…±å–å¾— {len(all_urls)} å€‹åŸå§‹ç¶²å€")

            # éæ¿¾é»‘åå–®
            filtered_urls = [url for url in all_urls if not is_blacklisted_url(url)]
            st.success(f"âœ… ç¶“ééæ¿¾å¾Œå‰©ä¸‹ {len(filtered_urls)} å€‹å¯ç–‘ç¶²å€")

            high_risk_urls = []

            for idx, url in enumerate(filtered_urls, start=1):
                st.markdown(f"---\n### ğŸ”— [{idx}/{len(filtered_urls)}] åˆ†æç¶²å€ï¼š[{url}]({url})")

                with st.spinner("â³ æ­£åœ¨åˆ†æ..."):
                    text_content = crawl_all_text(url)
                    text_result = chain.invoke(text_content)

                    image_urls = crawl_images(url)
                    flagged_images = 0

                    # åˆ†å…©æ¬„é¡¯ç¤ºæ–‡å­—èˆ‡åœ–åƒ
                    col1,  col2 = st.columns([5,  5])

                    with col1:
                        st.markdown("#### ğŸ“„ æ–‡å­—åˆ†é¡çµæœ")
                        st.write(text_result)


                    with col2:
                        st.markdown("#### ğŸ“· åœ–åƒåˆ†æçµæœ")
                        if not image_urls:
                            st.write("(æœªæ‰¾åˆ°åœ–ç‰‡)")
                        else:
                            for img in image_urls[:2]:
                                img_result = classify_image(img, llm_image)
                                st.image(img, caption=f"åˆ†é¡çµæœ: {img_result}")
                                if "Warning" in img_result:
                                    flagged_images += 1

                # ç¶œåˆåˆ¤æ–·
                if "(1)" in text_result or flagged_images > 0:
                    high_risk_urls.append(url)
                    st.error("âš ï¸ é«˜é¢¨éšªç¶²ç«™")
                else:
                    st.success("âœ… å®‰å…¨ç¶²ç«™")

            # ç¸½çµèˆ‡ä¸‹è¼‰
            st.markdown("---")
            st.subheader("ğŸ“‹ åˆ†æç¸½çµ")

            if high_risk_urls:
                st.warning(f"âš ï¸ åµæ¸¬åˆ°é«˜é¢¨éšªç¶²å€ï¼š{len(high_risk_urls)} ç­†")
                st.download_button(
                    label="ğŸ“¥ ä¸‹è¼‰é«˜é¢¨éšªç¶²å€æ¸…å–®",
                    data="\n".join(high_risk_urls),
                    file_name="google_high_risk_urls.txt",
                    mime="text/plain"
                )
            else:
                st.success("âœ… æ‰€æœ‰æœå°‹çµæœå‡æœªåµæ¸¬åˆ°é«˜é¢¨éšªå…§å®¹")

if __name__ == "__main__":
    main()














    
