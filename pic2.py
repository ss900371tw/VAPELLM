import streamlit as st
import requests
import os
import shutil
import time
import random
import re
from bs4 import BeautifulSoup
from dotenv import load_dotenv
from PIL import Image
from langchain_core.output_parsers import StrOutputParser
from langchain_core.messages import HumanMessage
from langchain_openai import ChatOpenAI
from langchain.prompts import PromptTemplate
import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin
import time
import pickle
from bs4 import BeautifulSoup
import requests
from urllib.parse import urljoin
from googleapiclient.discovery import build
from PIL import Image
from io import BytesIO
from langchain_core.messages import HumanMessage
from langchain_core.messages import AIMessage

import sys


# -------------------- 1. ç’°å¢ƒè®Šæ•¸ --------------------
load_dotenv()
openai_api_key = os.getenv("OPENAI_API_KEY","")


# -------------------- 2. Prompt --------------------
text_template = """
ä½œç‚ºä¸€ä½å°ˆæ¥­çš„é›»å­è¸äº¤æ˜“ç¶²ç«™åˆ†é¡å™¨ï¼Œ
ä½ å…·å‚™è¾¨è­˜ä¸‹åˆ— HTML å…§å®¹ä¸¦å°‡å…¶åˆ†é¡çš„èƒ½åŠ›ï¼š

è«‹ä¾æ“šä»¥ä¸‹å…©é¡é€²è¡Œåˆ†é¡ï¼š

(1) é›»å­è¸éŠ·å”®ç¶²ç«™

(2) éæ­¤é¡ç¶²ç«™

ç”±æ–¼ç”¢å“åç¨±ã€å“ç‰Œæˆ–æåŠæ–¹å¼ç„¡çª®ç„¡ç›¡ï¼Œè«‹å‹¿ä¾è³´ç‰¹å®šé—œéµå­—ï¼Œ
è€Œæ‡‰æ ¹æ“šé€šç”¨æŒ‡æ¨™åˆ¤æ–·ï¼Œç‰¹åˆ¥æ³¨æ„ä»¥ä¸‹å¹¾é»ï¼š

1ï¼‰ä½¿ç”¨æƒ…å¢ƒæˆ–å®£ç¨±å…§å®¹
ç•™æ„æ˜ç¤ºæˆ–æš—ç¤ºå…·ç²¾ç¥æ´»æ€§ã€è‡´å¹»æ•ˆæœã€æˆ–éæ³•ç”¨é€”çš„æè¿°ã€‚

è‹¥ç”¢å“æ¨™æ¦œæ”¹è®Šæƒ…ç·’ã€å¸¶ä¾†å¿«æ„Ÿã€å¼·åŠ›èˆ’å£“ç­‰éä¸€èˆ¬æ€§ç”¨é€”ï¼Œå¯èƒ½æš—ç¤ºå…¶å…·ç®¡åˆ¶å±¬æ€§ã€‚

2ï¼‰äº¤æ˜“æ€§ç·šç´¢
æåŠåƒ¹æ ¼ã€åƒæ˜¯ã€Œç§è¨Šæˆ‘ã€ã€ã€Œç¥•å¯†é…æ–¹ã€ã€ã€ŒåŠ  LINE è©³è«‡ã€ã€ã€Œéš±å¯†å‡ºè²¨ã€ç­‰æªè¾­ï¼Œ
å¯èƒ½èˆ‡éæ³•æˆ–å—é™ç”¢å“æœ‰é—œã€‚

è‹¥ç¶²ç«™æä¾›ï¼š

åŠ å…¥è³¼ç‰©è»Š

å¿«é€Ÿä¸‹å–®

ç”¢å“å‹éŒ„ç€è¦½

â†’ å³ä¾¿è¡¨é¢åˆæ³•ï¼Œä¹Ÿæ‡‰æé«˜è­¦è¦ºï¼Œå¯èƒ½å±¬æ–¼éæ³•è²©å”®ç¶²ç«™ã€‚

3ï¼‰æ³•å¾‹åˆ†é¡æˆ–è­¦ç¤ºè¨Šæ¯
æª¢æŸ¥æ˜¯å¦æœ‰é™åˆ¶è²©å”®ã€å¹´é½¡é©—è­‰ã€åƒ…é™æŒè­‰äººè³¼è²·ã€æˆ–åœ°å€æ€§éŠ·å”®é™åˆ¶ç­‰ã€‚

å°æ–¼æ¨¡ç³Šä¸æ¸…æˆ–è‡ªç›¸çŸ›ç›¾çš„è²æ˜ä¹Ÿè¦ä¿æŒè­¦æƒ•ã€‚

4ï¼‰ç”¢å“æˆ–æœå‹™é¡å‹
è‹¥ç”¢å“ç‚ºå·²çŸ¥ç®¡åˆ¶è—¥å“æˆ–è™•æ–¹è—¥å“ï¼ˆå¦‚ï¼šä½æ²›çœ ã€åœ°è¥¿æ³®ã€èŠ¬å¤ªå°¼ã€Kä»–å‘½ã€è´Šå®‰è«¾ã€æ›²é¦¬å¤šç­‰ï¼‰ï¼Œ
ä¸”æ­é…ä»¥ä¸‹åŠŸèƒ½ï¼š

åŠ å…¥è³¼ç‰©è»Š

å¿«é€Ÿè³¼è²·

SKU æˆ–å‹è™Ÿé¡¯ç¤º

æŸ¥è©¢åƒ¹æ ¼

æä¾›è³¼è²·èªªæ˜

â†’ ç„¡è«–è©²ç¶²ç«™çœ‹èµ·ä¾†å¤šéº¼å°ˆæ¥­ã€ç§‘å­¸æˆ–é†«ç™‚ï¼Œå‡æ‡‰åˆ¤å®šç‚º (1)

âš ï¸ æ³¨æ„ï¼šå°ˆæ¥­å¤–è§€èˆ‡ç§‘å­¸ç”¨èªç„¡æ³•æ©è“‹å…¶éŠ·å”®ç®¡åˆ¶ç‰©è³ªçš„äº‹å¯¦ã€‚

é›»å­è¸è£ç½®èˆ‡ç…™æ²¹ï¼ˆe-liquidï¼‰åŒæ¨£å±¬æ–¼ (1)ã€‚

è‹¥ç”¢å“ç‚ºéç®¡åˆ¶è£œå“ã€ä¸­è—¥æã€æˆ–ç„¡å®³è‰æœ¬ç”¢å“ï¼Œå‰‡å¯æ­¸ç‚º (2)ã€‚

5ï¼‰é—œéµæ€è€ƒ
ä¸è¦åªçœ‹å¤–è§€æˆ–å“ç‰ŒåŒ…è£ï¼Œæ›´é‡è¦çš„æ˜¯ç”¢å“å±¬æ€§èˆ‡æ˜¯å¦æä¾›ä¸‹å–®è³¼è²·æ©Ÿåˆ¶ã€‚

é‡é»åœ¨æ–¼ï¼š

å¯¦éš›è²©å”®çš„ç”¢å“æ˜¯å¦ç‚ºç®¡åˆ¶ç‰©è³ª

ç¶²ç«™æ˜¯å¦å…·æœ‰è³¼è²·åŠŸèƒ½æˆ–å¼•å°è³¼è²·è¡Œç‚º

6ï¼‰è³‡è¨Šæ€§æˆ–æ•™è‚²æ€§ç¶²ç«™
è‹¥ç¶²ç«™åƒ…æä¾›åƒè€ƒè³‡è¨Šï¼ˆå¦‚ï¼šç”¨é€”ã€å‰¯ä½œç”¨ã€äº¤äº’ä½œç”¨ã€åŠ‘é‡èªªæ˜ï¼‰ï¼Œ
ä¸”ç„¡æä¾›è³¼è²·ã€ä¸‹å–®ã€è¯çµ¡æ–¹å¼ç­‰åŠŸèƒ½ï¼Œæ‡‰æ­¸ç‚º (2)ã€‚

é€™é¡ç¶²ç«™åŒ…å«ï¼š

é†«å­¸ç™¾ç§‘

å¥åº·è³‡è¨Šå¹³å°

æ”¿åºœè³‡æº

è—¥å“è³‡æ–™åº«ï¼ˆå¦‚ï¼šdrugs.comã€WebMDã€NIHã€Mayo Clinicï¼‰

å³ä½¿æœ‰æåŠç®¡åˆ¶è—¥å“ï¼Œåªè¦ç„¡äº¤æ˜“ã€åƒ¹æ ¼ã€å‹è™Ÿæˆ–è¨‚è³¼è³‡è¨Šï¼Œå°±å±¬è³‡è¨Šæ€§ç”¨é€”ã€‚

ğŸ§  ç‰¹åˆ¥æ³¨æ„ï¼šè«‹å‹¿éŒ¯åˆ¤æ•™è‚²æ€§ç¶²ç«™ç‚ºè²©å”®ç¶²ç«™ã€‚

ğŸ”ã€å¾…åˆ†é¡ HTML å…§å®¹ã€‘ï¼š

{html}

ğŸ“‹ ç¶œåˆå ±å‘Šï¼š
æœ€çµ‚åˆ†é¡ï¼š(1) é›»å­è¸éŠ·å”®ç¶²ç«™  æˆ– (2) éæ­¤é¡ç¶²ç«™

å°è‡´æ­¤åˆ†é¡çš„é€šç”¨åˆ¤æ–·ä¾æ“šï¼ˆæ ¹æ“šä¸Šé¢ 1ï½6 æ¢ï¼‰

è‹¥æœ‰ä»»ä½•æ¨¡ç³Šè™•ï¼ˆä¾‹å¦‚çœ‹èµ·ä¾†å¾ˆå°ˆæ¥­ä½†å…¶å¯¦æœ‰è²©å”®è¡Œç‚ºï¼‰ï¼Œè«‹èªªæ˜ä½ çš„è™•ç†æ–¹å¼ã€‚


"""
prompt = PromptTemplate.from_template(template=text_template)

# -------------------- 3. çˆ¬å–ç¶²é æ–‡å­— --------------------








def crawl_all_text(url: str):
    try:
        response = requests.get(url, timeout=10)
        response.raise_for_status()
        soup = BeautifulSoup(response.text, 'html.parser')
        return soup.get_text(separator="\n", strip=True)[:50]
    except requests.exceptions.RequestException as e:
        return f"[Request failed]: {e}"


# ---------------------------------------------------------------------------
# 4. çˆ¬å–ç¶²é çš„åœ–ç‰‡ URL
# ---------------------------------------------------------------------------


def is_image_url(url):
    try:
        head = requests.head(url, timeout=5)
        content_type = head.headers.get("Content-Type", "")
        return content_type.startswith("image/")
    except:
        return False

def crawl_images(url: str):
    try:
        response = requests.get(url, timeout=10)
        response.raise_for_status()
        soup = BeautifulSoup(response.text, 'html.parser')
        img_tags = soup.find_all("img")

        valid_extensions = {".jpg", ".jpeg", ".png", ".webp"}
        seen = set()
        img_urls = []

        for img in img_tags:
            # å¾å¤šå€‹å¯èƒ½æ¬„ä½æ‰¾çœŸå¯¦åœ–ç‰‡ä¾†æº
            src_candidates = [
                img.get("src"),
                img.get("data-src"),
                img.get("data-original"),
                img.get("data-image"),
                img.get("data-lazy"),
            ]

            for src in src_candidates:
                if not src or src in seen:
                    continue
                seen.add(src)
                full_url = urljoin(url, src)
                lower_url = full_url.lower()

                # æœ‰å‰¯æª”åç›´æ¥æ¥å—
                if any(lower_url.endswith(ext) for ext in valid_extensions):
                    img_urls.append(full_url)
                    break
                # ç„¡å‰¯æª”åçš„åœ–ç‰‡ï¼Œé  HEAD åˆ¤æ–·
                elif is_image_url(full_url):
                    img_urls.append(full_url)
                    break

        return img_urls
    except Exception as e:
        print(f"[crawl_images error]: {e}")
        return []



# -------------------- 5. ä¸‹è¼‰åœ–ç‰‡ --------------------
def download_image(img_url, save_path="images"):
    if not os.path.exists(save_path):
        os.makedirs(save_path)

    img_name = os.path.join(save_path, os.path.basename(img_url.split("?")[0]))

    try:
        response = requests.get(img_url, stream=True, timeout=10)
        response.raise_for_status()
        with open(img_name, "wb") as out_file:
            shutil.copyfileobj(response.raw, out_file)
        return img_name
    except:
        return None


# -------------------- 6. åˆ†æåœ–ç‰‡ --------------------
def get_image_prompt(img_url: str) -> str:
    return f"""
è«‹æä¾›å¯¦éš›çš„åœ–ç‰‡æˆ–åœ–ç‰‡ç¶²å€ {img_url}ï¼Œæˆ‘æ‰èƒ½åˆ†æå…¶ä¸­æ˜¯å¦åŒ…å«ä»¥ä¸‹ä»»ä½•å—é™ç‰©å“æˆ–ç¬¦è™Ÿï¼š

é›»å­è¸è£ç½®æˆ–å…¶é›¶ä»¶

é›»å­ç…™å“ç‰Œæ¨™èªŒ

å¸ç…™ç…™éœ§æ•ˆæœ

ç®¡åˆ¶è—¥å“æˆ–å…¶ç”¨å…·

èˆ‡è—¥ç‰©æ–‡åŒ–ç›¸é—œçš„è¦–è¦ºå…ƒç´ 

æ¨å»£é›»å­è¸çš„è¡ŒéŠ·å…§å®¹ç­‰

ğŸ“· è«‹ä¸Šå‚³åœ–ç‰‡ï¼Œæˆ–æä¾›æœ‰æ•ˆçš„åœ–ç‰‡é€£çµï¼Œæˆ‘æœƒç«‹å³ç‚ºä½ åˆ¤å®šæ˜¯ï¼š

ğŸš¨ "Warning: Contains restricted items"

âœ… "Safe"

Image URL: {img_url}
"""

def classify_image(image_input, model):
    """
    image_input å¯ä»¥æ˜¯ï¼š
    - åœ–ç‰‡ç¶²å€ (str)
    - BytesIO åœ–ç‰‡è³‡æ–™ï¼ˆç›®å‰ä¸æ”¯æ´ï¼‰
    - æœ¬åœ°æª”æ¡ˆè·¯å¾‘ (str)
    model: ChatOpenAI é¡å‹æ¨¡å‹ï¼ˆå¦‚ gpt-4-vision-previewï¼‰
    """
    try:
        # å¦‚æœæ˜¯ç¶²å€
        if isinstance(image_input, str) and image_input.startswith("http"):
            message = HumanMessage(
                content=[
                    {"type": "text", "text": "è«‹åˆ¤æ–·é€™å¼µåœ–ç‰‡æ˜¯å¦åŒ…å«é›»å­è¸ã€æ¯’å“æˆ–ç›¸é—œç¬¦è™Ÿï¼Œå›å‚³ï¼šğŸš¨ Warning æˆ– âœ… Safe"},
                    {"type": "image_url", "image_url": {"url": image_input}},
                ]
            )
        elif isinstance(image_input, BytesIO):
            raise ValueError("LangChain ä¸æ”¯æ´ BytesIO åœ–ç‰‡è¼¸å…¥ï¼Œè«‹æ”¹ç”¨ OpenAI SDK")
        else:
            raise TypeError("ä¸æ”¯æ´çš„åœ–ç‰‡è¼¸å…¥é¡å‹")

        result = model.invoke([message])
        return result.content

    except Exception as e:
        return f"åœ–ç‰‡è®€å–æˆ–åˆ†æå¤±æ•—: {e}"

        
# -------------------- 7. Google Search --------------------
def google_search(query, count=10):
    api_key = os.getenv("GOOGLE_API_KEY")
    cx = os.getenv("GOOGLE_CX")
    if not api_key or not cx:
        print("âŒ GOOGLE_API_KEY æˆ– GOOGLE_CX æ²’æœ‰æ­£ç¢ºè¨­å®š")
        return []
    try:
        service = build("customsearch", "v1", developerKey=api_key)
        results = []
        fetched = 0
        while fetched < count:
            num = min(10, count - fetched)
            start = fetched + 1
            res = service.cse().list(q=query, cx=cx, num=num, start=start).execute()
            items = res.get("items", [])
            results.extend([item["link"] for item in items])
            fetched += len(items)
            if len(items) < num:
                break
        return results
    except Exception as e:
        print(f"âŒ Google æœå°‹éŒ¯èª¤ï¼š{e}")
        return []

# -------------------- 8. é»‘åå–® --------------------
blacklist_domains = [
    ".edu", ".gov", ".ac.", ".org", ".wiki", "usask.ca", "su.se", "article",
    "researchgate", "sciencedirect", "osf.io", "digitalcommons",
    "escholarship", "openai.com", "archive.org", "wiktionary",
    "urbandictionary", "dictionary", "bjc-r", "ecprice", "adamrose"
]
blacklist_keywords_in_url = [
    "slang", "street-names", "code-words", "download", "vocab", "wordlist",
    "unigrams", "passphrases", "pdf", "xml", "djvu.txt", "txt", "books",
    "raw/main", "viewcontent.cgi", "novel.pdf", "API/docs", "textfiles",
    "publications"
]

def is_blacklisted_url(url: str) -> bool:
    url_lower = url.lower()
    return any(domain in url_lower for domain in blacklist_domains) or \
           any(kw in url_lower for kw in blacklist_keywords_in_url)

# -------------------- 9. Streamlit ä¸»ç¨‹å¼ --------------------
def main():
    st.title("é›»å­è¸ç¶²ç«™åµæ¸¬ç³»çµ±")

    st.markdown("""
    <style>
        .stApp {
        background-image: url("https://wallpapers.com/images/hd/dark-grey-aesthetic-iy0yvgt4wq4qafgg.jpg");
        background-size: cover;
        background-repeat: no-repeat;
        background-attachment: fixed;
        }
        h1, h2, h3 {
            color: #00FFFF; /* Electric Blue */
        }
        .stRadio > div {
            flex-direction: row;
            gap: 2rem;
        }
        .stButton > button, .stDownloadButton > button {
            background-color: #3EB489; /* Mint Green */
            color: white;
            font-weight: bold;
            border-radius: 8px;
            padding: 0.5rem 1.2rem;
        }
        .stTextInput > div > input,
        .stTextArea > div > textarea {
            border-radius: 6px;
        }
    </style>
""", unsafe_allow_html=True)
    st.markdown(" ğŸ§  åˆ©ç”¨ OpenAI + åœ–ç‰‡è¾¨è­˜ï¼Œè‡ªå‹•åˆ†é¡é›»å­ç…™ç›¸é—œç¶²ç«™")
    
    llm_text = ChatOpenAI(api_key=openai_api_key, model="gpt-4o", temperature=0)
    llm_image = ChatOpenAI(api_key=openai_api_key, model="gpt-4.1", temperature=0)
    parser = StrOutputParser()
    chain = prompt | llm_text | parser

    # -------------------- æ¨¡å¼é¸æ“‡å€å¡Šï¼ˆç¾åŒ–ï¼‰ --------------------
    st.markdown("""
<div style="background-color:#f7f9fc;padding:1.2rem 1.5rem;border-radius:12px;border-left:6px solid #3EB489;">
    <h4 style="margin-bottom:0.5rem;">ğŸ›ï¸ æ¨¡å¼é¸æ“‡</h4>
    <p style="margin:0;">
    è«‹é¸æ“‡æ‚¨å¸Œæœ›ä½¿ç”¨çš„åˆ†ææ–¹å¼ï¼š
    <ul style="margin-top:0.5rem;margin-bottom:0;">
        <li>ğŸ” <b>å–®ä¸€ç¶²å€åˆ†æ</b>ï¼šåˆ†æå–®å€‹ç¶²ç«™çš„æ–‡å­—èˆ‡åœ–ç‰‡</li>
        <li>ğŸ“‚ <b>æ‰¹é‡ç¶²å€åˆ†æ</b>ï¼šä¸Šå‚³æ–‡å­—æª”ï¼Œä¸€æ¬¡åˆ†æå¤šå€‹ç¶²ç«™</li>
        <li>ğŸŒ <b>GOOGLE è‡ªå‹•æœå°‹</b>ï¼šæ ¹æ“šé—œéµå­—è‡ªå‹•æ‰¾ç¶²ç«™ä¸¦åˆ†æ</li>
    </ul>
    </p>
</div>
""", unsafe_allow_html=True)
    
    mode = st.selectbox(
    "ğŸ“Œ è«‹é¸æ“‡æ¨¡å¼",
    [
        "ğŸ” å–®ä¸€ç¶²å€åˆ†æ",
        "ğŸ“‚ æ‰¹é‡ç¶²å€åˆ†æ",
        "ğŸŒ GOOGLE è‡ªå‹•æœå°‹ & åˆ†æ"
    ]
)

    if "å–®ä¸€ç¶²å€åˆ†æ" in mode:
        st.markdown("### ğŸ”— å–®ä¸€ç¶²å€åˆ†æ")
        url = st.text_input("è«‹è¼¸å…¥ç¶²å€ï¼š")

        if st.button("ğŸš€ é–‹å§‹åˆ†æ"):
            if not url.strip():
                st.warning("âš ï¸ è«‹è¼¸å…¥æœ‰æ•ˆç¶²å€")
                return

            st.markdown(f"### ğŸ” æ­£åœ¨åˆ†æï¼š[{url}]({url})")

            with st.spinner("â³ æ­£åœ¨è®€å–ç¶²ç«™å…§å®¹èˆ‡åœ–ç‰‡..."):
                text_content = crawl_all_text(url)
                text_result = chain.invoke(text_content)

                image_urls = crawl_images(url)
                flagged_images = 0

                # åˆ†æˆå…©æ¬„é¡¯ç¤ºåˆ†æçµæœ
                col1,  col2 = st.columns([5,  5])

                with col1:
                        st.markdown(f"""
<div style="background-color:#f7f9fc;padding:1.2rem 1.5rem;border-radius:12px;border-left:6px solid #1f77b4;margin-bottom:1rem;">
    <h4 style="margin-bottom:0.8rem;">ğŸ“„ æ–‡å­—åˆ†é¡çµæœ</h4>
    <pre style="white-space:pre-wrap;font-size:0.92rem;font-family:inherit;">
{text_result}
    </pre>
</div>
""", unsafe_allow_html=True)
                with col2:
                    st.markdown("#### ğŸ“· åœ–åƒåˆ†æçµæœ")
                    if not image_urls:
                        st.write("(æœªæ‰¾åˆ°åœ–ç‰‡)")
                    else:
                        sample_size = min(2, len(image_urls))
                        for img in random.sample(image_urls, sample_size):
                            img_result = classify_image(img, llm_image)
                            st.image(img, caption=f"åˆ†é¡çµæœ: {img_result}")
                            if "Warning" in img_result:
                                flagged_images += 1



            st.markdown("---")
            st.subheader("ğŸ“‹ ç¶œåˆçµè«–")
            if "(1)" in text_result and flagged_images > 0:
                st.error("âš ï¸ é«˜é¢¨éšªç¶²ç«™ï¼šç¶²ç«™å¯èƒ½æ¶‰åŠé›»å­ç…™è²©å”®")
            if "(1)" in text_result:
                st.error("âš ï¸ é«˜é¢¨éšªç¶²ç«™ï¼šç¶²ç«™å¯èƒ½æ¶‰åŠé›»å­ç…™è²©å”®")
            else:
                st.success("âœ… å®‰å…¨ç¶²ç«™ï¼šæœªåµæ¸¬å‡ºé«˜é¢¨éšªå…§å®¹")

    elif "æ‰¹é‡ç¶²å€åˆ†æ" in mode:
        st.markdown("### ğŸ“‚ æ‰¹é‡ç¶²å€åˆ†æ")
        uploaded_file = st.file_uploader("è«‹ä¸Šå‚³ `.txt` æª”æ¡ˆï¼ˆæ¯è¡Œä¸€å€‹ç¶²å€ï¼‰", type=["txt"])

        if st.button("ğŸš€ é–‹å§‹æ‰¹æ¬¡åˆ†æ"):
            if uploaded_file is None:
                st.warning("âš ï¸ è«‹å…ˆä¸Šå‚³ `.txt` æª”æ¡ˆ")
                return

            urls = [line.strip().decode("utf-8") for line in uploaded_file.readlines() if line]
            st.info(f"ğŸ“„ å…±æœ‰ {len(urls)} å€‹ç¶²å€å°‡é€²è¡Œåˆ†æ")

            high_risk_urls = []

            for idx, url in enumerate(urls, start=1):
                st.markdown(f"---\n### ğŸ”— [{idx}/{len(urls)}] åˆ†æç¶²å€ï¼š[{url}]({url})")

                with st.spinner("â³ æ­£åœ¨åˆ†æ..."):
                    text_content = crawl_all_text(url)
                    text_result = chain.invoke(text_content)
                    image_urls = crawl_images(url)
                    flagged_images = 0

                    # å·¦å³åˆ†å€ï¼šæ–‡å­— / åœ–åƒ
                    col1,  col2 = st.columns([5, 5])


                    with col1:
                        st.markdown(f"""
<div style="background-color:#f7f9fc;padding:1.2rem 1.5rem;border-radius:12px;border-left:6px solid #1f77b4;margin-bottom:1rem;">
    <h4 style="margin-bottom:0.8rem;">ğŸ“„ æ–‡å­—åˆ†é¡çµæœ</h4>
    <pre style="white-space:pre-wrap;font-size:0.92rem;font-family:inherit;">
{text_result}
    </pre>
</div>
""", unsafe_allow_html=True)


                    with col2:
                        if not image_urls:
                            st.markdown("""
        <div style="background-color:#f7f9fc;padding:1.2rem 1.5rem;
                    border-radius:12px;border-left:6px solid #ff7f0e;
                    margin-bottom:1rem;">
            <h4 style="margin-bottom:0.8rem;">ğŸ“· åœ–åƒåˆ†æçµæœ</h4>
            <p>(æœªæ‰¾åˆ°åœ–ç‰‡)</p>
        </div>
        """, unsafe_allow_html=True)
                        else:
                            # é–‹å§‹å¤–æ¡†
                            st.markdown("""
        <div style="background-color:#f7f9fc;padding:1.2rem 1.5rem;
                    border-radius:12px;border-left:6px solid #ff7f0e;
                    margin-bottom:1rem;">
            <h4 style="margin-bottom:0.8rem;">ğŸ“· åœ–åƒåˆ†æçµæœ</h4>
        """, unsafe_allow_html=True)

                            # é¡¯ç¤ºåœ–ç‰‡èˆ‡çµæœ
                            for img in image_urls[:2]:
                                img_result = classify_image(img, llm_image)
                                st.image(img, caption=f"åˆ†é¡çµæœï¼š{img_result}", use_container_width =True)
                                if "Warning" in img_result:
                                    flagged_images += 1
    
                            # çµæŸå¤–æ¡†
                            st.markdown("</div>", unsafe_allow_html=True)
                st.markdown("---")
                # ç¶œåˆçµè«–
                if "(1)" in text_result and flagged_images > 0:
                    high_risk_urls.append(url)
                    st.error("âš ï¸ é«˜é¢¨éšªç¶²ç«™ï¼šç¶²ç«™å¯èƒ½æ¶‰åŠé›»å­ç…™è²©å”®")
                if "(1)" in text_result:
                    high_risk_urls.append(url)
                    st.error("âš ï¸ é«˜é¢¨éšªç¶²ç«™ï¼šç¶²ç«™å¯èƒ½æ¶‰åŠé›»å­ç…™è²©å”®")
                else:
                    st.success("âœ… å®‰å…¨ç¶²ç«™")

            st.markdown("---")
            st.subheader("ğŸ“‹ æ‰¹æ¬¡åˆ†æç¸½çµ")

            if high_risk_urls:
                st.warning(f"âš ï¸ å…±åµæ¸¬åˆ°é«˜é¢¨éšªç¶²å€ {len(high_risk_urls)} ç­†")

                st.download_button(
                    label="ğŸ“¥ ä¸‹è¼‰é«˜é¢¨éšªç¶²å€æ¸…å–®",
                    data="\n".join(high_risk_urls),
                    file_name="high_risk_urls.txt",
                    mime="text/plain"
                )
            else:
                st.success("âœ… æ‰€æœ‰ç¶²å€çš†æœªåµæ¸¬åˆ°é«˜é¢¨éšªå…§å®¹")

    else:
        st.markdown("### ğŸŒ Google æœå°‹åˆ†ææ¨¡å¼")
        st.markdown("> æ ¹æ“šé—œéµå­—è‡ªå‹•æœå°‹ç¶²ç«™ï¼Œä¸¦å°æ¯å€‹æœå°‹çµæœé€²è¡Œ AI æ–‡æœ¬èˆ‡åœ–åƒåˆ¤æ–·")

        # è¼¸å…¥é—œéµå­—
        keywords_text = st.text_area(
            "ğŸ”¤ è«‹è¼¸å…¥æœå°‹é—œéµå­—ï¼ˆæ¯è¡Œä¸€å€‹ï¼‰",
            "vape\ne-juice\ne-cigarette\né›»å­ç…™"
        )

        limit = st.number_input("ğŸ”¢ æ¯å€‹é—œéµå­—æœ€å¤šæ“·å–å¹¾çµ„ç¶²å€ï¼Ÿ", min_value=1, max_value=50, value=10)

        if st.button("ğŸš€ åŸ·è¡Œ Google æœå°‹ä¸¦åˆ†æ"):
            if not keywords_text.strip():
                st.warning("âš ï¸ è«‹å…ˆè¼¸å…¥é—œéµå­—")
                return

            keywords_list = [kw.strip() for kw in keywords_text.split("\n") if kw.strip()]
            st.info(f"ğŸ” å°‡é‡å° {len(keywords_list)} å€‹é—œéµå­—ï¼Œå„æ“·å– {limit} çµ„æœå°‹çµæœ")

            all_urls = []
            for kw in keywords_list:
                st.markdown(f"#### ğŸ” æœå°‹é—œéµå­—ï¼š**{kw}**")
                found = google_search(kw, count=limit)
                all_urls.extend([url for url in found if url not in all_urls])

            st.write(f"ğŸ“¥ ç¸½å…±å–å¾— {len(all_urls)} å€‹åŸå§‹ç¶²å€")

            # éæ¿¾é»‘åå–®
            filtered_urls = [url for url in all_urls if not is_blacklisted_url(url)]
            st.success(f"âœ… ç¶“ééæ¿¾å¾Œå‰©ä¸‹ {len(filtered_urls)} å€‹å¯ç–‘ç¶²å€")

            high_risk_urls = []

            for idx, url in enumerate(filtered_urls, start=1):
                st.markdown(f"---\n### ğŸ”— [{idx}/{len(filtered_urls)}] åˆ†æç¶²å€ï¼š[{url}]({url})")

                with st.spinner("â³ æ­£åœ¨åˆ†æ..."):
                    text_content = crawl_all_text(url)
                    text_result = chain.invoke(text_content)

                    image_urls = crawl_images(url)
                    flagged_images = 0

                    # åˆ†å…©æ¬„é¡¯ç¤ºæ–‡å­—èˆ‡åœ–åƒ
                    col1,  col2 = st.columns([5,  5])

                    with col1:
                        st.markdown("#### ğŸ“„ æ–‡å­—åˆ†é¡çµæœ")
                        st.write(text_result)


                    with col2:
                        if not image_urls:
                            st.markdown("""
        <div style="background-color:#f7f9fc;padding:1.2rem 1.5rem;
                    border-radius:12px;border-left:6px solid #ff7f0e;
                    margin-bottom:1rem;">
            <h4 style="margin-bottom:0.8rem;">ğŸ“· åœ–åƒåˆ†æçµæœ</h4>
            <p>(æœªæ‰¾åˆ°åœ–ç‰‡)</p>
        </div>
        """, unsafe_allow_html=True)
                        else:
                            # é–‹å§‹å¤–æ¡†
                            st.markdown("""
        <div style="background-color:#f7f9fc;padding:1.2rem 1.5rem;
                    border-radius:12px;border-left:6px solid #ff7f0e;
                    margin-bottom:1rem;">
            <h4 style="margin-bottom:0.8rem;">ğŸ“· åœ–åƒåˆ†æçµæœ</h4>
        """, unsafe_allow_html=True)

                            # é¡¯ç¤ºåœ–ç‰‡èˆ‡çµæœ
                            for img in image_urls[:2]:
                                img_result = classify_image(img, llm_image)
                                st.image(img, caption=f"åˆ†é¡çµæœï¼š{img_result}", use_container_width =True)
                                if "Warning" in img_result:
                                    flagged_images += 1
    
                            # çµæŸå¤–æ¡†
                            st.markdown("</div>", unsafe_allow_html=True)
                st.markdown("---")
                # ç¶œåˆåˆ¤æ–·
                if "(1)" in text_result and flagged_images > 0:
                    high_risk_urls.append(url)
                    st.error("âš ï¸ é«˜é¢¨éšªç¶²ç«™ï¼šç¶²ç«™å¯èƒ½æ¶‰åŠé›»å­ç…™è²©å”®")
                if "(1)" in text_result:
                    high_risk_urls.append(url)
                    st.error("âš ï¸ é«˜é¢¨éšªç¶²ç«™ï¼šç¶²ç«™å¯èƒ½æ¶‰åŠé›»å­ç…™è²©å”®")
                else:
                    st.success("âœ… å®‰å…¨ç¶²ç«™")

            # ç¸½çµèˆ‡ä¸‹è¼‰
            st.markdown("---")
            st.subheader("ğŸ“‹ åˆ†æç¸½çµ")

            if high_risk_urls:
                st.warning(f"âš ï¸ åµæ¸¬åˆ°é«˜é¢¨éšªç¶²å€ï¼š{len(high_risk_urls)} ç­†")
                st.download_button(
                    label="ğŸ“¥ ä¸‹è¼‰é«˜é¢¨éšªç¶²å€æ¸…å–®",
                    data="\n".join(high_risk_urls),
                    file_name="google_high_risk_urls.txt",
                    mime="text/plain"
                )
            else:
                st.success("âœ… æ‰€æœ‰æœå°‹çµæœå‡æœªåµæ¸¬åˆ°é«˜é¢¨éšªå…§å®¹")

if __name__ == "__main__":
    main()














    
