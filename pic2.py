import streamlit as st
import requests
import os
import re
from bs4 import BeautifulSoup
from dotenv import load_dotenv
import ollama
from googleapiclient.discovery import build

# ---------------------------------------------------------------------------
# 1. åˆå§‹åŒ–
# ---------------------------------------------------------------------------
load_dotenv()
llm_client = ollama.Client()

# ---------------------------------------------------------------------------
# 2. HTML åˆ†ææç¤ºæ¨¡æ¿
# ---------------------------------------------------------------------------
text_template = """
As an expert classifier of e-cigarette and controlled-substance-selling websites,
you are equipped to analyze the following HTML and categorize it as either:
(1) an e-cigarette/controlled-substance-selling website,
(2) not such a website.

Because there are infinitely many potential products, brand names, or references, you will use
general indicators rather than item-specific keywords. In particular, consider the following:

1) Context of Use or Claims:
   - Psychoactive/mind-altering effects?
   - "High", "relieve stress", unusual health claims?

2) Transactional Cues:
   - "Price", "add to cart", "order now", "discreet shipping", "contact me"?

3) Legal Notes:
   - Age gating, regional restrictions, vague disclaimers?

4) Product Type:
   - Controlled substances (e.g., Diazepam, Tramadol) + Add to Cart = classify as (1)
   - Vape/e-cigarette = classify as (1)
   - Supplements or harmless herbal = (2)

5) Don't be fooled by scientific appearance if sale enabled.

6) Educational/Medical:
   - Only discuss (no order/contact) = (2)

---
HTML content to classify:
{html}

---
Comprehensive Report:
- Final classification: (1) or (2)
- What general indicators led to your conclusion?
- If there are ambiguities, explain how resolved.
"""

# ---------------------------------------------------------------------------
# 3. æœ¬åœ° OLLAMA åˆ†æå‡½æ•¸
# ---------------------------------------------------------------------------
def classify_text_with_ollama(text: str, model_name="gemma3"):
    prompt_filled = text_template.replace("{html}", text)
    response = llm_client.chat(
        model=model_name,
        messages=[{"role": "user", "content": prompt_filled}]
    )
    return response['message']['content']

def get_image_prompt(img_url: str) -> str:
    return f"""
Analyze this image and determine whether it contains any of the following:
ğŸš¨ **If the image contains ANY of the following, classify it as "Warning: Contains restricted items"** ğŸš¨

- E-cigarettes, vape pens, e-liquids, vape tanks or parts
- Controlled substances (weed, pills, cocaine, heroin, etc.)
- Drug-use paraphernalia (bongs, rolling papers, syringes)
- Visuals like ğŸ, "420", psychedelic symbols

âœ… If NONE of the above are present, classify it as "Safe".

Image URL: {img_url}
"""

def classify_image(img_url: str, model_name='llama3.2-vision:11b'):
    prompt = get_image_prompt(img_url)
    response = llm_client.chat(
        model=model_name,
        messages=[{"role": "user", "content": prompt}]
    )
    return response['message']['content']

# ---------------------------------------------------------------------------
# 4. æŠ“å–æ–‡å­—èˆ‡åœ–ç‰‡
# ---------------------------------------------------------------------------
def crawl_all_text(url: str):
    try:
        response = requests.get(url, timeout=10)
        response.raise_for_status()
        soup = BeautifulSoup(response.text, 'html.parser')
        return soup.get_text(separator="\n", strip=True)[:5000]
    except:
        return "[Request failed]"

def crawl_images(url: str):
    try:
        response = requests.get(url, timeout=10)
        response.raise_for_status()
        soup = BeautifulSoup(response.text, 'html.parser')
        return [requests.compat.urljoin(url, img.get("src"))
                for img in soup.find_all("img")
                if img.get("src") and img.get("src").lower().endswith(('.jpg', '.jpeg', '.png', '.webp'))]
    except:
        return []

# ---------------------------------------------------------------------------
# 5. é»‘åå–®è¦å‰‡
# ---------------------------------------------------------------------------
blacklist_domains = [".edu", ".gov", ".ac.", ".org", ".wiki", "openai.com"]
blacklist_keywords_in_url = ["dictionary", "slang", "download", "passphrases", "pdf", "viewcontent.cgi"]

def is_blacklisted_url(url: str) -> bool:
    url = url.lower()
    return any(d in url for d in blacklist_domains) or any(k in url for k in blacklist_keywords_in_url)

# ---------------------------------------------------------------------------
# 6. Google æœå°‹åŠŸèƒ½
# ---------------------------------------------------------------------------
def google_search(query, count=10):
    api_key = os.getenv("GOOGLE_API_KEY")
    cx = os.getenv("GOOGLE_CX")
    try:
        service = build("customsearch", "v1", developerKey=api_key)
        results = []
        fetched = 0
        while fetched < count:
            num = min(10, count - fetched)
            start = fetched + 1
            res = service.cse().list(q=query, cx=cx, num=num, start=start).execute()
            results += [item["link"] for item in res.get("items", [])]
            fetched += len(res.get("items", []))
            if len(res.get("items", [])) < num:
                break
        return results
    except:
        return []

# ---------------------------------------------------------------------------
# 7. åˆ†æå–®ä¸€ç¶²å€
# ---------------------------------------------------------------------------
def analyze_url(url):
    text = crawl_all_text(url)
    result = classify_text_with_ollama(text)
    st.write("ğŸ“„ **æ–‡å­—åˆ†æçµæœï¼š**")
    st.write(result)

    flagged = 0
    images = crawl_images(url)
    for img in images[:2]:  # é™åˆ¶å‰å…©å¼µ
        img_result = classify_image(img)
        st.image(img, caption=img_result)
        if "Warning" in img_result:
            flagged += 1

    if "(1)" in result:
        st.success("âš ï¸ åˆ¤å®šï¼šé«˜é¢¨éšªç¶²ç«™")
    elif flagged > 0:
        st.warning("âš ï¸ åœ–ç‰‡å«ç–‘ä¼¼å…§å®¹ï¼Œéœ€è­¦è¦º")
    else:
        st.info("âœ… åˆ¤å®šï¼šç„¡é¢¨éšª")

# ---------------------------------------------------------------------------
# 8. Streamlit ä¸»ä»‹é¢
# ---------------------------------------------------------------------------
def main():
    st.title("ğŸ” é›»å­è¸ / ç®¡åˆ¶è—¥å“ç¶²ç«™è¾¨è­˜å·¥å…· (æœ¬åœ° OLLAMA)")
    mode = st.radio("è«‹é¸æ“‡æ¨¡å¼ï¼š", ["å–®ä¸€ç¶²å€åˆ†æ", "æ‰¹é‡ç¶²å€åˆ†æ", "Google æœå°‹åˆ†æ"])

    if mode == "å–®ä¸€ç¶²å€åˆ†æ":
        url = st.text_input("è¼¸å…¥ç¶²å€ï¼š")
        if st.button("é–‹å§‹åˆ†æ") and url:
            analyze_url(url)

    elif mode == "æ‰¹é‡ç¶²å€åˆ†æ":
        uploaded = st.file_uploader("ä¸Šå‚³ TXT æª”ï¼ˆæ¯è¡Œä¸€å€‹ç¶²å€ï¼‰", type="txt")
        if st.button("é–‹å§‹åˆ†æ") and uploaded:
            urls = [line.strip().decode("utf-8") for line in uploaded.readlines() if line.strip()]
            for url in urls:
                st.subheader(url)
                analyze_url(url)

    elif mode == "Google æœå°‹åˆ†æ":
        kw_text = st.text_area("è¼¸å…¥é—œéµå­—ï¼ˆæ¯è¡Œä¸€å€‹ï¼‰", "vape\ne-cigarette\né›»å­ç…™")
        limit = st.number_input("æ¯é—œéµå­—æ“·å–å¹¾ç­†çµæœï¼Ÿ", min_value=1, max_value=50, value=10)
        if st.button("é–‹å§‹æœå°‹èˆ‡åˆ†æ"):
            kws = [k.strip() for k in kw_text.splitlines() if k.strip()]
            all_urls = []
            for kw in kws:
                st.write(f"ğŸ” æœå°‹é—œéµå­—ï¼š{kw}")
                found = google_search(kw, count=limit)
                all_urls.extend(found)

            filtered = [u for u in all_urls if not is_blacklisted_url(u)]
            st.write(f"ğŸ§ª å…± {len(filtered)} ç­†éé»‘åå–®ç¶²å€å°‡è¢«åˆ†æ")
            for idx, url in enumerate(filtered):
                st.subheader(f"[{idx+1}] {url}")
                analyze_url(url)

if __name__ == "__main__":
    main()







    
