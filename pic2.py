import streamlit as st
import requests
import os
import shutil
import time
import random
import re
from bs4 import BeautifulSoup
from dotenv import load_dotenv
from PIL import Image
from googleapiclient.discovery import build
from langchain_core.output_parsers import StrOutputParser
from langchain_core.messages import HumanMessage
from langchain_openai import ChatOpenAI
from langchain.prompts import PromptTemplate
from urllib.parse import urljoin
import pickle
from googleapiclient.discovery import build
from PIL import Image
from io import BytesIO
from langchain_core.messages import AIMessage
from playwright.sync_api import sync_playwright
import sys
from urllib.parse import urlparse
from playwright.sync_api import sync_playwright
from urllib.parse import urlparse
import pickle
from bs4 import BeautifulSoup
import requests
import time
import sys
from bs4 import BeautifulSoup
import time
import undetected_chromedriver as uc

load_dotenv()
openai_api_key = os.getenv("OPENAI_API_KEY","")



# -------------------- 2. Prompt --------------------
text_template = """
ä½œç‚ºä¸€ä½å°ˆæ¥­çš„é›»å­è¸äº¤æ˜“ç¶²ç«™åˆ†é¡å™¨ï¼Œ
ä½ å…·å‚™è¾¨è­˜ä¸‹åˆ— HTML å…§å®¹ä¸¦å°‡å…¶åˆ†é¡çš„èƒ½åŠ›ï¼š

è«‹ä¾æ“šä»¥ä¸‹å…©é¡é€²è¡Œåˆ†é¡ï¼š

(1) é›»å­è¸éŠ·å”®ç¶²ç«™

(2) éæ­¤é¡ç¶²ç«™

ç”±æ–¼ç”¢å“åç¨±ã€å“ç‰Œæˆ–æåŠæ–¹å¼ç„¡çª®ç„¡ç›¡ï¼Œè«‹å‹¿ä¾è³´ç‰¹å®šé—œéµå­—ï¼Œ
è€Œæ‡‰æ ¹æ“šé€šç”¨æŒ‡æ¨™åˆ¤æ–·ï¼Œç‰¹åˆ¥æ³¨æ„ä»¥ä¸‹å¹¾é»ï¼š

1ï¼‰ä½¿ç”¨æƒ…å¢ƒæˆ–å®£ç¨±å…§å®¹
ç•™æ„æ˜ç¤ºæˆ–æš—ç¤ºå…·ç²¾ç¥æ´»æ€§ã€è‡´å¹»æ•ˆæœã€æˆ–éæ³•ç”¨é€”çš„æè¿°ã€‚

è‹¥ç”¢å“æ¨™æ¦œæ”¹è®Šæƒ…ç·’ã€å¸¶ä¾†å¿«æ„Ÿã€å¼·åŠ›èˆ’å£“ç­‰éä¸€èˆ¬æ€§ç”¨é€”ï¼Œå¯èƒ½æš—ç¤ºå…¶å…·ç®¡åˆ¶å±¬æ€§ã€‚

2ï¼‰äº¤æ˜“æ€§ç·šç´¢
æåŠåƒ¹æ ¼ã€åƒæ˜¯ã€Œç§è¨Šæˆ‘ã€ã€ã€Œç¥•å¯†é…æ–¹ã€ã€ã€ŒåŠ  LINE è©³è«‡ã€ã€ã€Œéš±å¯†å‡ºè²¨ã€ç­‰æªè¾­ï¼Œ
å¯èƒ½èˆ‡éæ³•æˆ–å—é™ç”¢å“æœ‰é—œã€‚

è‹¥ç¶²ç«™æä¾›ï¼š

åŠ å…¥è³¼ç‰©è»Š

å¿«é€Ÿä¸‹å–®

ç”¢å“å‹éŒ„ç€è¦½

â†’ å³ä¾¿è¡¨é¢åˆæ³•ï¼Œä¹Ÿæ‡‰æé«˜è­¦è¦ºï¼Œå¯èƒ½å±¬æ–¼éæ³•è²©å”®ç¶²ç«™ã€‚

3ï¼‰æ³•å¾‹åˆ†é¡æˆ–è­¦ç¤ºè¨Šæ¯
æª¢æŸ¥æ˜¯å¦æœ‰é™åˆ¶è²©å”®ã€å¹´é½¡é©—è­‰ã€åƒ…é™æŒè­‰äººè³¼è²·ã€æˆ–åœ°å€æ€§éŠ·å”®é™åˆ¶ç­‰ã€‚

å°æ–¼æ¨¡ç³Šä¸æ¸…æˆ–è‡ªç›¸çŸ›ç›¾çš„è²æ˜ä¹Ÿè¦ä¿æŒè­¦æƒ•ã€‚

4ï¼‰ç”¢å“æˆ–æœå‹™é¡å‹
è‹¥ç”¢å“ç‚ºå·²çŸ¥ç®¡åˆ¶è—¥å“æˆ–è™•æ–¹è—¥å“ï¼ˆå¦‚ï¼šä½æ²›çœ ã€åœ°è¥¿æ³®ã€èŠ¬å¤ªå°¼ã€Kä»–å‘½ã€è´Šå®‰è«¾ã€æ›²é¦¬å¤šç­‰ï¼‰ï¼Œ
ä¸”æ­é…ä»¥ä¸‹åŠŸèƒ½ï¼š

åŠ å…¥è³¼ç‰©è»Š

å¿«é€Ÿè³¼è²·

SKU æˆ–å‹è™Ÿé¡¯ç¤º

æŸ¥è©¢åƒ¹æ ¼

æä¾›è³¼è²·èªªæ˜

â†’ ç„¡è«–è©²ç¶²ç«™çœ‹èµ·ä¾†å¤šéº¼å°ˆæ¥­ã€ç§‘å­¸æˆ–é†«ç™‚ï¼Œå‡æ‡‰åˆ¤å®šç‚º (1)

âš ï¸ æ³¨æ„ï¼šå°ˆæ¥­å¤–è§€èˆ‡ç§‘å­¸ç”¨èªç„¡æ³•æ©è“‹å…¶éŠ·å”®ç®¡åˆ¶ç‰©è³ªçš„äº‹å¯¦ã€‚

é›»å­è¸è£ç½®èˆ‡ç…™æ²¹ï¼ˆe-liquidï¼‰åŒæ¨£å±¬æ–¼ (1)ã€‚

è‹¥ç”¢å“ç‚ºéç®¡åˆ¶è£œå“ã€ä¸­è—¥æã€æˆ–ç„¡å®³è‰æœ¬ç”¢å“ï¼Œå‰‡å¯æ­¸ç‚º (2)ã€‚

5ï¼‰é—œéµæ€è€ƒ
ä¸è¦åªçœ‹å¤–è§€æˆ–å“ç‰ŒåŒ…è£ï¼Œæ›´é‡è¦çš„æ˜¯ç”¢å“å±¬æ€§èˆ‡æ˜¯å¦æä¾›ä¸‹å–®è³¼è²·æ©Ÿåˆ¶ã€‚

é‡é»åœ¨æ–¼ï¼š

å¯¦éš›è²©å”®çš„ç”¢å“æ˜¯å¦ç‚ºç®¡åˆ¶ç‰©è³ª

ç¶²ç«™æ˜¯å¦å…·æœ‰è³¼è²·åŠŸèƒ½æˆ–å¼•å°è³¼è²·è¡Œç‚º

6ï¼‰è³‡è¨Šæ€§æˆ–æ•™è‚²æ€§ç¶²ç«™
è‹¥ç¶²ç«™åƒ…æä¾›åƒè€ƒè³‡è¨Šï¼ˆå¦‚ï¼šç”¨é€”ã€å‰¯ä½œç”¨ã€äº¤äº’ä½œç”¨ã€åŠ‘é‡èªªæ˜ï¼‰ï¼Œ
ä¸”ç„¡æä¾›è³¼è²·ã€ä¸‹å–®ã€è¯çµ¡æ–¹å¼ç­‰åŠŸèƒ½ï¼Œæ‡‰æ­¸ç‚º (2)ã€‚

é€™é¡ç¶²ç«™åŒ…å«ï¼š

é†«å­¸ç™¾ç§‘

å¥åº·è³‡è¨Šå¹³å°

æ”¿åºœè³‡æº

è—¥å“è³‡æ–™åº«ï¼ˆå¦‚ï¼šdrugs.comã€WebMDã€NIHã€Mayo Clinicï¼‰

å³ä½¿æœ‰æåŠç®¡åˆ¶è—¥å“ï¼Œåªè¦ç„¡äº¤æ˜“ã€åƒ¹æ ¼ã€å‹è™Ÿæˆ–è¨‚è³¼è³‡è¨Šï¼Œå°±å±¬è³‡è¨Šæ€§ç”¨é€”ã€‚

ğŸ§  ç‰¹åˆ¥æ³¨æ„ï¼šè«‹å‹¿éŒ¯åˆ¤æ•™è‚²æ€§ç¶²ç«™ç‚ºè²©å”®ç¶²ç«™ã€‚

ğŸ”ã€å¾…åˆ†é¡ HTML å…§å®¹ã€‘ï¼š

{html}

ğŸ“‹ ç¶œåˆå ±å‘Šï¼š
æœ€çµ‚åˆ†é¡ï¼š(1) é›»å­è¸éŠ·å”®ç¶²ç«™  æˆ– (2) éæ­¤é¡ç¶²ç«™

å°è‡´æ­¤åˆ†é¡çš„é€šç”¨åˆ¤æ–·ä¾æ“šï¼ˆæ ¹æ“šä¸Šé¢ 1ï½6 æ¢ï¼‰

è‹¥æœ‰ä»»ä½•æ¨¡ç³Šè™•ï¼ˆä¾‹å¦‚çœ‹èµ·ä¾†å¾ˆå°ˆæ¥­ä½†å…¶å¯¦æœ‰è²©å”®è¡Œç‚ºï¼‰ï¼Œè«‹èªªæ˜ä½ çš„è™•ç†æ–¹å¼ã€‚


"""
prompt = PromptTemplate.from_template(template=text_template)

# -------------------- 3. çˆ¬å–ç¶²é æ–‡å­— --------------------


import subprocess
import os

# ç¢ºä¿ç€è¦½å™¨å·²å®‰è£ï¼ˆåƒ…é¦–æ¬¡éƒ¨ç½²æœƒè§¸ç™¼ï¼‰
def ensure_playwright_browser():
    chromium_path = os.path.expanduser("~/.cache/ms-playwright/chromium-*/chrome-linux/chrome")
    if not os.path.exists(chromium_path):
        try:
            print("âš™ï¸ å®‰è£ Playwright ç€è¦½å™¨ä¸­...")
            subprocess.run(["playwright", "install", "chromium"], check=True)
        except Exception as e:
            print("âŒ Playwright å®‰è£å¤±æ•—:", e)

ensure_playwright_browser()

from urllib.parse import urlparse
import pickle
import time
from bs4 import BeautifulSoup
import requests
from requests_html import HTMLSession

from playwright.sync_api import sync_playwright
from urllib.parse import urlparse
import pickle
from bs4 import BeautifulSoup
import requests
import time

from playwright.sync_api import sync_playwright
from urllib.parse import urlparse
import pickle
from bs4 import BeautifulSoup
import requests
import time



import requests
from bs4 import BeautifulSoup
from urllib.parse import urlparse
import pickle
import asyncio
from playwright.async_api import async_playwright


import requests
from bs4 import BeautifulSoup
from urllib.parse import urlparse
import pickle
import time
import undetected_chromedriver as uc  # âœ… ä¿ç•™é€™å€‹ï¼Œä¸ä½¿ç”¨ä¹Ÿä¸åˆªé™¤
import asyncio
from playwright.async_api import async_playwright


from bs4 import BeautifulSoup
import requests
import pickle
import time
from urllib.parse import urlparse
from playwright.sync_api import sync_playwright


import requests
from bs4 import BeautifulSoup
import pickle
import time
from urllib.parse import urlparse
from playwright.sync_api import sync_playwright
import undetected_chromedriver as uc

import os
import requests
import pickle
import time
from bs4 import BeautifulSoup
from urllib.parse import urlparse
from playwright.sync_api import sync_playwright
import undetected_chromedriver as uc

from playwright.sync_api import sync_playwright
from urllib.parse import urlparse
import pickle
from bs4 import BeautifulSoup
import requests



import asyncio
from playwright.async_api import async_playwright
from bs4 import BeautifulSoup

import asyncio
from playwright.async_api import async_playwright
from bs4 import BeautifulSoup

from bs4 import BeautifulSoup
from urllib.parse import urlparse
import undetected_chromedriver as uc
import pickle
import time


import undetected_chromedriver as uc
import time
import pickle

import asyncio
from playwright.async_api import async_playwright
from bs4 import BeautifulSoup
from urllib.parse import urlparse
import os

import asyncio
from playwright.async_api import async_playwright
from bs4 import BeautifulSoup
from urllib.parse import urlparse
import os

# save_cookie.py
import asyncio
from playwright.async_api import async_playwright
import pickle
import os

from seleniumbase import SB
import requests
from bs4 import BeautifulSoup
from urllib.parse import urlparse
import pickle
import time
from playwright.sync_api import sync_playwright

import requests
from bs4 import BeautifulSoup
from urllib.parse import urlparse
from playwright.sync_api import sync_playwright
import pickle
import os



def crawl_all_text(url: str, cookie_file: str = "cookies.pkl"):
    try:
        response = requests.get(url, timeout=10)
        response.raise_for_status()
        soup = BeautifulSoup(response.text, 'html.parser')
        return soup.get_text(separator="\n", strip=True)[:50]

    except requests.exceptions.RequestException as e:
        if "403" in str(e):
            print("âš ï¸ HTTP 403 Forbidden - åˆ‡æ›ç‚º Playwright ç¹éé©—è­‰")

            try:
                with sync_playwright() as p:
                    browser = p.chromium.launch(headless=False)  # å»ºè­° debug æ™‚å…ˆç”¨é headless
                    context = browser.new_context()

                    parsed = urlparse(url)
                    base_url = f"{parsed.scheme}://{parsed.netloc}/"

                    # å…ˆé–‹é¦–é ï¼Œå»ºç«‹ domain context
                    page = context.new_page()
                    page.goto(base_url, timeout=30000)
                    time.sleep(3)

                    # è¼‰å…¥ cookiesï¼ˆå¦‚æœ‰ï¼‰
                    try:
                        with open(cookie_file, "rb") as f:
                            cookies = pickle.load(f)
                            for cookie in cookies:
                                if 'domain' not in cookie:
                                    cookie['domain'] = "." + parsed.hostname
                            context.add_cookies(cookies)
                    except Exception as err:
                        print("âš ï¸ Cookie è¼‰å…¥å¤±æ•—:", err)

                    # è·³è½‰åˆ°ç›®æ¨™é é¢
                    page.goto(url, timeout=30000)
                    time.sleep(5)
                    html = page.content()
                    browser.close()

                    soup = BeautifulSoup(html, "html.parser")
                    for tag in soup(["script", "style"]):
                        tag.decompose()

                    body_text = soup.get_text(separator="\n", strip=True)
                    if "é©—è­‰æ‚¨æ˜¯äººé¡" in body_text or "Enable JavaScript and cookies to continue" in body_text:
                        return "[âš ï¸ Cloudflare Verification Failed] Cookie å¯èƒ½å¤±æ•ˆæˆ–æœªæ­£ç¢ºé™„åŠ "

                    return body_text[:50]

            except Exception as e:
                return f"{url}"



# ---------------------------------------------------------------------------
# 4. çˆ¬å–ç¶²é çš„åœ–ç‰‡ URL
# ---------------------------------------------------------------------------


import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin

def is_image_url(url: str) -> bool:
    try:
        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
                          "AppleWebKit/537.36 (KHTML, like Gecko) "
                          "Chrome/114.0.0.0 Safari/537.36"
        }
        resp = requests.head(url, headers=headers, timeout=5, allow_redirects=True)
        return resp.headers.get("Content-Type", "").lower().startswith("image/")
    except:
        return False

def normalize_src(src: str, base_url: str) -> str:
    if not src:
        return ""
    if src.startswith("//"):
        return "https:" + src
    return urljoin(base_url, src)

def crawl_images(url: str):
    try:
        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
                          "AppleWebKit/537.36 (KHTML, like Gecko) "
                          "Chrome/114.0.0.0 Safari/537.36"
        }
        response = requests.get(url, headers=headers, timeout=10)
        response.raise_for_status()
        soup = BeautifulSoup(response.text, 'html.parser')
        img_tags = soup.find_all("img")

        valid_extensions = {".jpg", ".jpeg", ".png", ".webp"}
        seen = set()
        img_urls = []

        for img in img_tags:
            src_candidates = [
                img.get("src"),
                img.get("data-src"),
                img.get("data-original"),
                img.get("data-image"),
                img.get("data-lazy"),
            ]

            for src in src_candidates:
                if not src:
                    continue
                full_url = normalize_src(src, url)
                if full_url in seen or len(full_url) < 10 or "base64" in full_url:
                    continue
                seen.add(full_url)

                lower_url = full_url.lower()
                if any(lower_url.endswith(ext) for ext in valid_extensions):
                    img_urls.append(full_url)
                    break
                elif is_image_url(full_url):
                    img_urls.append(full_url)
                    break

        return img_urls
    except Exception as e:
        print(f"[crawl_images error]: {e}")
        return []



# -------------------- 5. ä¸‹è¼‰åœ–ç‰‡ --------------------
def download_image(img_url, save_path="images"):
    if not os.path.exists(save_path):
        os.makedirs(save_path)

    img_name = os.path.join(save_path, os.path.basename(img_url.split("?")[0]))

    try:
        response = requests.get(img_url, stream=True, timeout=10)
        response.raise_for_status()
        with open(img_name, "wb") as out_file:
            shutil.copyfileobj(response.raw, out_file)
        return img_name
    except:
        return None


# -------------------- 6. åˆ†æåœ–ç‰‡ --------------------
def get_image_prompt(img_url: str) -> str:
    return f"""
è«‹æä¾›å¯¦éš›çš„åœ–ç‰‡æˆ–åœ–ç‰‡ç¶²å€ {img_url}ï¼Œæˆ‘æ‰èƒ½åˆ†æå…¶ä¸­æ˜¯å¦åŒ…å«ä»¥ä¸‹ä»»ä½•å—é™ç‰©å“æˆ–ç¬¦è™Ÿï¼š

é›»å­è¸è£ç½®æˆ–å…¶é›¶ä»¶

é›»å­ç…™å“ç‰Œæ¨™èªŒ

å¸ç…™ç…™éœ§æ•ˆæœ

ç®¡åˆ¶è—¥å“æˆ–å…¶ç”¨å…·

èˆ‡è—¥ç‰©æ–‡åŒ–ç›¸é—œçš„è¦–è¦ºå…ƒç´ 

æ¨å»£é›»å­è¸çš„è¡ŒéŠ·å…§å®¹ç­‰

ğŸ“· è«‹ä¸Šå‚³åœ–ç‰‡ï¼Œæˆ–æä¾›æœ‰æ•ˆçš„åœ–ç‰‡é€£çµï¼Œæˆ‘æœƒç«‹å³ç‚ºä½ åˆ¤å®šæ˜¯ï¼š

ğŸš¨ "Warning: Contains restricted items"

âœ… "Safe"

Image URL: {img_url}
"""

def classify_image(image_input, model):
    """
    image_input å¯ä»¥æ˜¯ï¼š
    - åœ–ç‰‡ç¶²å€ (str)
    - BytesIO åœ–ç‰‡è³‡æ–™ï¼ˆç›®å‰ä¸æ”¯æ´ï¼‰
    - æœ¬åœ°æª”æ¡ˆè·¯å¾‘ (str)
    model: ChatOpenAI é¡å‹æ¨¡å‹ï¼ˆå¦‚ gpt-4-vision-previewï¼‰
    """
    try:
        # å¦‚æœæ˜¯ç¶²å€
        if isinstance(image_input, str) and image_input.startswith("http"):
            message = HumanMessage(
                content=[
                    {"type": "text", "text": "è«‹åˆ¤æ–·é€™å¼µåœ–ç‰‡æ˜¯å¦åŒ…å«é›»å­è¸ã€æ¯’å“æˆ–ç›¸é—œç¬¦è™Ÿï¼Œå›å‚³ï¼šğŸš¨ Warning æˆ– âœ… Safe"},
                    {"type": "image_url", "image_url": {"url": image_input}},
                ]
            )
        elif isinstance(image_input, BytesIO):
            raise ValueError("LangChain ä¸æ”¯æ´ BytesIO åœ–ç‰‡è¼¸å…¥ï¼Œè«‹æ”¹ç”¨ OpenAI SDK")
        else:
            raise TypeError("ä¸æ”¯æ´çš„åœ–ç‰‡è¼¸å…¥é¡å‹")

        result = model.invoke([message])
        return result.content

    except Exception as e:
        return f"åœ–ç‰‡è®€å–æˆ–åˆ†æå¤±æ•—: {e}"

        
# -------------------- 7. Google Search --------------------
def google_search(query, count=10):
    api_key = os.getenv("GOOGLE_API_KEY")
    cx = os.getenv("GOOGLE_CX")
    if not api_key or not cx:
        print("âŒ GOOGLE_API_KEY æˆ– GOOGLE_CX æ²’æœ‰æ­£ç¢ºè¨­å®š")
        return []
    try:
        service = build("customsearch", "v1", developerKey=api_key)
        results = []
        fetched = 0
        while fetched < count:
            num = min(10, count - fetched)
            start = fetched + 1
            res = service.cse().list(q=query, cx=cx, num=num, start=start).execute()
            items = res.get("items", [])
            results.extend([item["link"] for item in items])
            fetched += len(items)
            if len(items) < num:
                break
        return results
    except Exception as e:
        print(f"âŒ Google æœå°‹éŒ¯èª¤ï¼š{e}")
        return []

# -------------------- 8. é»‘åå–® --------------------
blacklist_domains = [
    ".edu", ".gov", ".ac.", ".org", ".wiki", "usask.ca", "su.se", "article",
    "researchgate", "sciencedirect", "osf.io", "digitalcommons",
    "escholarship", "openai.com", "archive.org", "wiktionary",
    "urbandictionary", "dictionary", "bjc-r", "ecprice", "adamrose"
]
blacklist_keywords_in_url = [
    "slang", "street-names", "code-words", "download", "vocab", "wordlist",
    "unigrams", "passphrases", "pdf", "xml", "djvu.txt", "txt", "books",
    "raw/main", "viewcontent.cgi", "novel.pdf", "API/docs", "textfiles",
    "publications"
]

def is_blacklisted_url(url: str) -> bool:
    url_lower = url.lower()
    return any(domain in url_lower for domain in blacklist_domains) or \
           any(kw in url_lower for kw in blacklist_keywords_in_url)
    
# -------------------- 9. Streamlit ä¸»ç¨‹å¼ --------------------
def main():
    st.markdown("<h1 style='text-align:center;'>é›»å­è¸ç¶²ç«™åµæ¸¬ç³»çµ±</h1>", unsafe_allow_html=True)

    # èƒŒæ™¯æ¨£å¼èˆ‡ä¸»é¡Œæ–‡å­—
    st.markdown("""
    <style>
        .stApp {
            background-image: url("https://wallpapers.com/images/hd/dark-grey-aesthetic-iy0yvgt4wq4qafgg.jpg");
            background-size: cover;
            background-repeat: no-repeat;
            background-attachment: fixed;
        }
        h1, h2, h3 {
            color: #00FFFF;
        }
        .stButton > button {
            background-color: #3EB489;
            color: white;
            font-weight: bold;
            border-radius: 8px;
            padding: 0.5rem 1.2rem;
        }
    </style>
    """, unsafe_allow_html=True)

    st.markdown("""
    <p style='text-align:center; font-size: 24px;'>ğŸ§  åˆ©ç”¨ OpenAI + åœ–ç‰‡è¾¨è­˜ï¼Œè‡ªå‹•åˆ†é¡é›»å­ç…™ç›¸é—œç¶²ç«™</p>
    """, unsafe_allow_html=True)

    # åˆå§‹åŒ–
    if "selected_mode" not in st.session_state:
        st.session_state.selected_mode = None

    # é¡¯ç¤ºå¡ç‰‡
    
    def render_card(icon, title, desc, key):
        selected = st.session_state.get("selected_mode") == title
    
        border = "4px solid #3EB489" if selected else "1px solid #999999"
        shadow = "0 0 20px #3EB489" if selected else "none"
        bg = "#0c1b2a" if selected else "#1a1f2b"
    
        with st.container():
            # ç”¨ç©ºå­—ä¸²ä½”ä½ï¼Œè®“ button å‡ºç¾åœ¨ HTML block è£¡
            st.markdown(f"""
            <div style="
                background-color: {bg};
                color: white;
                border-radius: 16px;
                border: {border};
                box-shadow: {shadow};
                padding: 1.5rem;
                text-align: center;
                margin-bottom: 0.5rem;
            ">
                <div style="font-size: 2rem;">{icon}</div>
                <div style="font-size: 1.2rem; font-weight: bold; margin-top: 0.5rem;">{title}</div>
                <div style="font-size: 0.9rem; color: #ccc; margin-top: 0.3rem;">{desc}</div>
            """, unsafe_allow_html=True)
    
            # çœŸæ­£çš„ Streamlit æŒ‰éˆ•ï¼šæ¸²æŸ“åœ¨å¡ç‰‡å…§éƒ¨
            if st.button("é¸æ“‡", key=f"{key}_button"):
                st.session_state.selected_mode = title
    
            # é—œé–‰å¡ç‰‡å€å¡Š
            st.markdown("</div>", unsafe_allow_html=True)


                    

    # æ¨¡å¼é¸æ“‡
    st.markdown("## ğŸ“Œ è«‹é¸æ“‡åˆ†ææ¨¡å¼")
    col1, col2, col3 = st.columns(3)
    with col1:
        render_card("ğŸ”", "å–®ä¸€ç¶²å€åˆ†æ", "åˆ†æå–®å€‹ç¶²ç«™çš„æ–‡å­—èˆ‡åœ–ç‰‡", "card1")
    with col2:
        render_card("ğŸ“‚", "æ‰¹é‡ç¶²å€åˆ†æ", "ä¸Šå‚³æ–‡å­—æª”ï¼Œåˆ†æå¤šå€‹ç¶²ç«™", "card2")
    with col3:
        render_card("ğŸ“", "é—œéµå­—æœå°‹åˆ†æ", "æ ¹æ“šé—œéµå­—çˆ¬èŸ²åˆ†æ", "card3")


    # é¡¯ç¤ºé¸æ“‡æ¨¡å¼
    mode = st.session_state.selected_mode



    
    if mode:
        st.markdown(f"""
        <div style="background-color:#f7f9fc;padding:1rem 1.5rem;border-radius:12px;border-left:6px solid #3EB489;margin-top:1rem;">
            <h4 style="margin-bottom:0rem;">ğŸ¯ ç›®å‰é¸æ“‡çš„æ¨¡å¼ï¼š<span style="color:#3EB489;">{mode}</span></h4>
        </div>
        """, unsafe_allow_html=True)
    else:
        st.info("ğŸ‘‰ è«‹é»é¸ä¸Šæ–¹å¡ç‰‡ä¾†é¸æ“‡æ¨¡å¼")
    
    if mode:
        st.markdown(f"### ğŸ¯ é¸æ“‡æ¨¡å¼ï¼š**{mode}**")
    
        if "å–®ä¸€ç¶²å€åˆ†æ" in mode:
            st.markdown("### ğŸ”— å–®ä¸€ç¶²å€åˆ†æ")
            url = st.text_input("è«‹è¼¸å…¥ç¶²å€ï¼š")
    
            if st.button("ğŸš€ é–‹å§‹åˆ†æ"):
                if not url.strip():
                    st.warning("âš ï¸ è«‹è¼¸å…¥æœ‰æ•ˆç¶²å€")
                    return
    
                st.markdown(f"### ğŸ” æ­£åœ¨åˆ†æï¼š[{url}]({url})")
    
                with st.spinner("â³ æ­£åœ¨è®€å–ç¶²ç«™å…§å®¹èˆ‡åœ–ç‰‡..."):
                    text_content = crawl_all_text(url)
                    text_result = chain.invoke(text_content)
    
                    image_urls = crawl_images(url)
                    flagged_images = 0
    
                    # åˆ†æˆå…©æ¬„é¡¯ç¤ºåˆ†æçµæœ
                    col1,  col2 = st.columns([5,  5])
    
                    with col1:
                        st.markdown(f"""
    <div style="background-color:#f7f9fc;padding:1.2rem 1.5rem;border-radius:12px;border-left:6px solid #1f77b4;margin-bottom:1rem;">
        <h4 style="margin-bottom:0.8rem;">ğŸ“„ æ–‡å­—åˆ†é¡çµæœ</h4>
        <pre style="white-space:pre-wrap;font-size:0.92rem;font-family:inherit;">
    {text_result} 
        </pre>
    </div>
    """, unsafe_allow_html=True)
                    with col2:
                        if not image_urls:
                            st.markdown(f"""
    <div style="background-color:#f7f9fc;padding:1.2rem 1.5rem;border-radius:12px;border-left:6px solid #ff7f0e;margin-bottom:1rem;">
        <h4 style="margin-bottom:0.8rem;">ğŸ“· åœ–åƒåˆ†æçµæœ</h4>
        <div style="font-size:0.9rem;"><b>(æœªæ‰¾åˆ°åœ–ç‰‡)</b></div>
    </div>
    """, unsafe_allow_html=True)
                        else:
                            sample_size = min(2, len(image_urls))
                            for img in random.sample(image_urls, sample_size):
                                img_result = classify_image(img, llm_image)
                                st.markdown(f"""
    <div style="background-color:#f7f9fc;padding:1.2rem 1.5rem;border-radius:12px;border-left:6px solid #ff7f0e;margin-bottom:1rem;">
        <h4 style="margin-bottom:0.8rem;">ğŸ“· åœ–åƒåˆ†æçµæœ</h4>
        <img src="{img}" style="max-width:100%;border-radius:8px;margin-bottom:0.5rem;">
        <div style="font-size:0.9rem;"><b>åˆ†é¡çµæœï¼š</b>{img_result}</div>
    </div>""", unsafe_allow_html=True)
                                if "Warning" in img_result:
                                    flagged_images += 1
    
    
    
                st.markdown("---")
                st.subheader("ğŸ“‹ ç¶œåˆçµè«–")
                if "(1)" in text_result and flagged_images > 0:
                    st.error("âš ï¸ é«˜é¢¨éšªç¶²ç«™ï¼šç¶²ç«™å¯èƒ½æ¶‰åŠé›»å­ç…™è²©å”®")
                if "(1)" in text_result:
                    st.error("âš ï¸ é«˜é¢¨éšªç¶²ç«™ï¼šç¶²ç«™å¯èƒ½æ¶‰åŠé›»å­ç…™è²©å”®")
                else:
                    st.success("âœ… å®‰å…¨ç¶²ç«™ï¼šæœªåµæ¸¬å‡ºé«˜é¢¨éšªå…§å®¹")
    
        elif "æ‰¹é‡ç¶²å€åˆ†æ" in mode:
            st.markdown("### ğŸ“‚ æ‰¹é‡ç¶²å€åˆ†æ")
            uploaded_file = st.file_uploader("è«‹ä¸Šå‚³ .txt æª”æ¡ˆï¼ˆæ¯è¡Œä¸€å€‹ç¶²å€ï¼‰", type=["txt"])
    
            if st.button("ğŸš€ é–‹å§‹æ‰¹æ¬¡åˆ†æ"):
                if uploaded_file is None:
                    st.warning("âš ï¸ è«‹å…ˆä¸Šå‚³ .txt æª”æ¡ˆ")
                    return
    
                urls = [line.strip().decode("utf-8") for line in uploaded_file.readlines() if line]
                st.info(f"ğŸ“„ å…±æœ‰ {len(urls)} å€‹ç¶²å€å°‡é€²è¡Œåˆ†æ")
    
                high_risk_urls = []
    
                for idx, url in enumerate(urls, start=1):
                    st.markdown(f"---\n### ğŸ”— [{idx}/{len(urls)}] åˆ†æç¶²å€ï¼š[{url}]({url})")
    
                    with st.spinner("â³ æ­£åœ¨åˆ†æ..."):
                        text_content = crawl_all_text(url)
                        text_result = chain.invoke(text_content)
                        image_urls = crawl_images(url)
                        flagged_images = 0
    
                        # å·¦å³åˆ†å€ï¼šæ–‡å­— / åœ–åƒ
                        col1,  col2 = st.columns([5, 5])
    
                    with col1:
                        st.markdown(f"""
    <div style="background-color:#f7f9fc;padding:1.2rem 1.5rem;border-radius:12px;border-left:6px solid #1f77b4;margin-bottom:1rem;">
        <h4 style="margin-bottom:0.8rem;">ğŸ“„ æ–‡å­—åˆ†é¡çµæœ</h4>
        <pre style="white-space:pre-wrap;font-size:0.92rem;font-family:inherit;">
    {text_result}
        </pre>
    </div>
    """, unsafe_allow_html=True)
                    with col2:
                        if not image_urls:
                            st.markdown(f"""
    <div style="background-color:#f7f9fc;padding:1.2rem 1.5rem;border-radius:12px;border-left:6px solid #ff7f0e;margin-bottom:1rem;">
        <h4 style="margin-bottom:0.8rem;">ğŸ“· åœ–åƒåˆ†æçµæœ</h4>
        <div style="font-size:0.9rem;"><b>(æœªæ‰¾åˆ°åœ–ç‰‡)</b></div>
    </div>
    """, unsafe_allow_html=True)
                        else:
                            sample_size = min(2, len(image_urls))
                            for img in random.sample(image_urls, sample_size):
                                img_result = classify_image(img, llm_image)
                                st.markdown(f"""
    <div style="background-color:#f7f9fc;padding:1.2rem 1.5rem;border-radius:12px;border-left:6px solid #ff7f0e;margin-bottom:1rem;">
        <h4 style="margin-bottom:0.8rem;">ğŸ“· åœ–åƒåˆ†æçµæœ</h4>
        <img src="{img}" style="max-width:100%;border-radius:8px;margin-bottom:0.5rem;">
        <div style="font-size:0.9rem;"><b>åˆ†é¡çµæœï¼š</b>{img_result}</div>
    </div>
    """, unsafe_allow_html=True)
                                if "Warning" in img_result:
                                    flagged_images += 1
    
    
                    st.markdown("---")
                    # ç¶œåˆåˆ¤æ–·
                    if "(1)" in text_result and flagged_images > 0:
                        high_risk_urls.append(url)
                        st.error("âš ï¸ é«˜é¢¨éšªç¶²ç«™ï¼šç¶²ç«™å¯èƒ½æ¶‰åŠé›»å­ç…™è²©å”®")
                    if "(1)" in text_result:
                        high_risk_urls.append(url)
                        st.error("âš ï¸ é«˜é¢¨éšªç¶²ç«™ï¼šç¶²ç«™å¯èƒ½æ¶‰åŠé›»å­ç…™è²©å”®")
                    else:
                        st.success("âœ… å®‰å…¨ç¶²ç«™")
                st.markdown("---")
                st.subheader("ğŸ“‹ æ‰¹æ¬¡åˆ†æç¸½çµ")
                
                if high_risk_urls:
                    st.warning(f"âš ï¸ å…±åµæ¸¬åˆ°é«˜é¢¨éšªç¶²å€ {len(high_risk_urls)} ç­†")
    
                    st.download_button(
                        label="ğŸ“¥ ä¸‹è¼‰é«˜é¢¨éšªç¶²å€æ¸…å–®",
                        data="\n".join(high_risk_urls),
                        file_name="high_risk_urls.txt",
                        mime="text/plain"
                    )
                else:
                    st.success("âœ… æ‰€æœ‰ç¶²å€çš†æœªåµæ¸¬åˆ°é«˜é¢¨éšªå…§å®¹")
    
        else:
            st.markdown("### ğŸŒ Google æœå°‹åˆ†ææ¨¡å¼")
            st.markdown("> æ ¹æ“šé—œéµå­—è‡ªå‹•æœå°‹ç¶²ç«™ï¼Œä¸¦å°æ¯å€‹æœå°‹çµæœé€²è¡Œ AI æ–‡æœ¬èˆ‡åœ–åƒåˆ¤æ–·")
    
            # è¼¸å…¥é—œéµå­—
            keywords_text = st.text_area(
                "ğŸ”¤ è«‹è¼¸å…¥æœå°‹é—œéµå­—ï¼ˆæ¯è¡Œä¸€å€‹ï¼‰",
                "vape\ne-juice\ne-cigarette\né›»å­ç…™"
            )
    
            limit = st.number_input("ğŸ”¢ æ¯å€‹é—œéµå­—æœ€å¤šæ“·å–å¹¾çµ„ç¶²å€ï¼Ÿ", min_value=1, max_value=50, value=10)
    
            if st.button("ğŸš€ åŸ·è¡Œ Google æœå°‹ä¸¦åˆ†æ"):
                if not keywords_text.strip():
                    st.warning("âš ï¸ è«‹å…ˆè¼¸å…¥é—œéµå­—")
                    return
    
                keywords_list = [kw.strip() for kw in keywords_text.split("\n") if kw.strip()]
                st.info(f"ğŸ” å°‡é‡å° {len(keywords_list)} å€‹é—œéµå­—ï¼Œå„æ“·å– {limit} çµ„æœå°‹çµæœ")
    
                all_urls = []
                for kw in keywords_list:
                    st.markdown(f"#### ğŸ” æœå°‹é—œéµå­—ï¼š**{kw}**")
                    found = google_search(kw, count=limit)
                    all_urls.extend([url for url in found if url not in all_urls])
    
                st.write(f"ğŸ“¥ ç¸½å…±å–å¾— {len(all_urls)} å€‹åŸå§‹ç¶²å€")
    
                # éæ¿¾é»‘åå–®
                filtered_urls = [url for url in all_urls if not is_blacklisted_url(url)]
                st.success(f"âœ… ç¶“ééæ¿¾å¾Œå‰©ä¸‹ {len(filtered_urls)} å€‹å¯ç–‘ç¶²å€")
    
                high_risk_urls = []
    
                for idx, url in enumerate(filtered_urls, start=1):
                    st.markdown(f"---\n### ğŸ”— [{idx}/{len(filtered_urls)}] åˆ†æç¶²å€ï¼š[{url}]({url})")
    
                    with st.spinner("â³ æ­£åœ¨åˆ†æ..."):
                        text_content = crawl_all_text(url)
                        text_result = chain.invoke(text_content)
    
                        image_urls = crawl_images(url)
                        flagged_images = 0
    
                        # åˆ†å…©æ¬„é¡¯ç¤ºæ–‡å­—èˆ‡åœ–åƒ
                        col1,  col2 = st.columns([5,  5])
    
                    with col1:
                        st.markdown(f"""
    <div style="background-color:#f7f9fc;padding:1.2rem 1.5rem;border-radius:12px;border-left:6px solid #1f77b4;margin-bottom:1rem;">
        <h4 style="margin-bottom:0.8rem;">ğŸ“„ æ–‡å­—åˆ†é¡çµæœ</h4>
        <pre style="white-space:pre-wrap;font-size:0.92rem;font-family:inherit;">
    {text_result}
        </pre>
    </div>
    """, unsafe_allow_html=True)
                    with col2:
                        if not image_urls:
                            st.markdown(f"""
    <div style="background-color:#f7f9fc;padding:1.2rem 1.5rem;border-radius:12px;border-left:6px solid #ff7f0e;margin-bottom:1rem;">
        <h4 style="margin-bottom:0.8rem;">ğŸ“· åœ–åƒåˆ†æçµæœ</h4>
        <div style="font-size:0.9rem;"><b>(æœªæ‰¾åˆ°åœ–ç‰‡)</b></div>
    </div>
    """, unsafe_allow_html=True)
                        else:
                            sample_size = min(2, len(image_urls))
                            for img in random.sample(image_urls, sample_size):
                                img_result = classify_image(img, llm_image)
                                st.markdown(f"""
    <div style="background-color:#f7f9fc;padding:1.2rem 1.5rem;border-radius:12px;border-left:6px solid #ff7f0e;margin-bottom:1rem;">
        <h4 style="margin-bottom:0.8rem;">ğŸ“· åœ–åƒåˆ†æçµæœ</h4>
        <img src="{img}" style="max-width:100%;border-radius:8px;margin-bottom:0.5rem;">
        <div style="font-size:0.9rem;"><b>åˆ†é¡çµæœï¼š</b>{img_result}</div>
    </div>
    """, unsafe_allow_html=True)
                                if "Warning" in img_result:
                                    flagged_images += 1
    
    
    
                    
                    st.markdown("---")
                    # ç¶œåˆåˆ¤æ–·
                    if "(1)" in text_result and flagged_images > 0:
                        high_risk_urls.append(url)
                        st.error("âš ï¸ é«˜é¢¨éšªç¶²ç«™ï¼šç¶²ç«™å¯èƒ½æ¶‰åŠé›»å­ç…™è²©å”®")
                    if "(1)" in text_result:
                        high_risk_urls.append(url)
                        st.error("âš ï¸ é«˜é¢¨éšªç¶²ç«™ï¼šç¶²ç«™å¯èƒ½æ¶‰åŠé›»å­ç…™è²©å”®")
                    else:
                        st.success("âœ… å®‰å…¨ç¶²ç«™")
    
                # ç¸½çµèˆ‡ä¸‹è¼‰
                st.markdown("---")
                st.subheader("ğŸ“‹ åˆ†æç¸½çµ")
    
                if high_risk_urls:
                    st.warning(f"âš ï¸ åµæ¸¬åˆ°é«˜é¢¨éšªç¶²å€ï¼š{len(high_risk_urls)} ç­†")
                    st.download_button(
                        label="ğŸ“¥ ä¸‹è¼‰é«˜é¢¨éšªç¶²å€æ¸…å–®",
                        data="\n".join(high_risk_urls),
                        file_name="google_high_risk_urls.txt",
                        mime="text/plain"
                    )
                else:
                    st.success("âœ… æ‰€æœ‰æœå°‹çµæœå‡æœªåµæ¸¬åˆ°é«˜é¢¨éšªå…§å®¹")

if __name__ == "__main__":
    main()


